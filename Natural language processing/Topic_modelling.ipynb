{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Roku0_Dk2kpG"
      },
      "outputs": [],
      "source": [
        "#NLP\n",
        "#DATA ACQUISITION -> DATA PREPROCESSING -> WORD TO VEC -> MODEL BUILDING -> EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8uBPrJhYCcaV"
      },
      "outputs": [],
      "source": [
        "# POS TAGGING USING SPACY\n",
        "#POS TAGGING WILL BE HELPFUL FOR LEMMATIZATION\n",
        "#LEMME WORD WILL BE MORE ACCURATE WITH POS_TAG\n",
        "#EX- FLYING WILL BE LEMME TO FLYING WIHTOUT POS_TAG AND WIHT POS_TAG AS VERB IT WILL GIVE FLY\n",
        "#EX2-LEAVES WILL GIVE LEAVE NOT LEAF\n",
        "import spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TiqroARqC_nV"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')#loading small pipeline from spacy\n",
        "doc = nlp('i read books on NLP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34W7DztADTG1",
        "outputId": "1b9577bf-7c3f-4e08-d89c-151968289f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRON pronoun\n",
            "VERB verb\n",
            "NOUN noun\n",
            "ADP adposition\n",
            "PROPN proper noun\n"
          ]
        }
      ],
      "source": [
        "#fine grained pos have pos_,tag_\n",
        "\n",
        "for i in doc:\n",
        "  print(i.pos_,spacy.explain(i.pos_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHpQAlOlDXJK"
      },
      "outputs": [],
      "source": [
        "#POS TAGGGING CAN BE USED FOR NER\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "le =WordNetLemmatizer()\n",
        "st =PorterStemmer()\n",
        "print(le.lemmatize('flying'))\n",
        "st.stem('sitting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "jwJoGZV0klav",
        "outputId": "98e814f3-80bd-452e-9ab8-56a6ac22ab01"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flying\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "xo7jvgIJIgxE"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "ner =spacy.load(\"en_core_web_sm\")\n",
        "sentence = \"I am working at Infosys and My name is Avinash\"\n",
        "\n",
        "# Process the sentence with the model\n",
        "doc = ner(sentence)\n",
        "#doc=nlp(sent)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc.ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DETa7Xxny0D",
        "outputId": "53e5ed11-a6ab-4d7b-da5d-0f4265b47111"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Infosys, Avinash)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "  print(ent.text,'-',ent.label_)\n",
        "  print(spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R65vNjLej0wN",
        "outputId": "0b8bdce4-8953-4f47-b748-6da512e5c9cd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infosys - ORG\n",
            "Companies, agencies, institutions, etc.\n",
            "Avinash - PERSON\n",
            "People, including fictional\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI5PKMu4It0H",
        "outputId": "1d9a32f4-8b41-402d-e47a-f2e24e6a9fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "European - NORP\n",
            "Nationalities or religious or political groups\n",
            "Google - ORG\n",
            "Companies, agencies, institutions, etc.\n",
            "$5.1 billion - MONEY\n",
            "Monetary values, including unit\n",
            "Wednesday - DATE\n",
            "Absolute or relative dates or periods\n"
          ]
        }
      ],
      "source": [
        "for ent in doc.ents:\n",
        "  print(ent.text,\"-\",ent.label_)\n",
        "  print(spacy.explain(ent.label_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2spxbzKPAbf",
        "outputId": "3c45a68b-7a9d-4260-c2b2-7cca383868fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Infosys', 'ORG'), ('Avinash', 'PERSON')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "texts = [\n",
        "    \"I am going to Infosys.My name is Avinash\",\n",
        "]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "for doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
        "    # Do something with the doc here\n",
        "    print([(ent.text, ent.label_) for ent in doc.ents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6prdWTr5QNPq",
        "outputId": "5a75000c-482b-4d7c-b973-659a003dd1bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$9.4 million MONEY\n",
            "the prior year DATE\n",
            "$2.7 million MONEY\n",
            "twelve billion dollars MONEY\n",
            "1b MONEY\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "texts = \"Net income was $9.4 million compared to the prior year of $2.7 million.Revenue exceeded twelve billion dollars, with a loss of $1b.\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#for doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
        "doc = nlp(texts)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em0dSIonRY64",
        "outputId": "69c4cc59-af7c-465e-dcb0-0b9e6dd03adc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('$9.4 million', 'MONEY'), ('the prior year', 'DATE'), ('$2.7 million', 'MONEY')]\n",
            "[('twelve billion dollars', 'MONEY'), ('1b', 'MONEY')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "texts = [\n",
        "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
        "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
        "]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "for doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
        "    # Do something with the doc here\n",
        "    print([(ent.text, ent.label_) for ent in doc.ents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "xBhhdTjGLuVL",
        "outputId": "b5ae0f5d-65bb-4416-e038-1437e915946c"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-78fe05b33d48>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ORG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'English' object has no attribute 'explain'"
          ]
        }
      ],
      "source": [
        "#FOR NAMED ENTITY RECGONITOPN WE HAVE SPACY\n",
        "#IN SPACY WE LOAD EN_CORE_WEB_SM\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmFC0TOeIyIM",
        "outputId": "e38d0517-7829-4b96-f21b-8f9a456ba565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mark Zuckerberg  -  PERSON\n",
            "2004  -  DATE\n",
            "Menlo Park  -  GPE\n",
            "California  -  GPE\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a sample sentence\n",
        "sentence = \"Mark Zuckerberg founded Facebook in 2004 in Menlo Park, California.\"\n",
        "\n",
        "# Process the sentence with the model\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Iterate over the named entities in the document\n",
        "for ent in doc.ents:\n",
        "    # Print the entity text and label\n",
        "    print(ent.text,\" - \", ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#topic modelling is process of breaking down your documents into set of topics\n",
        "#for example if you have 1000 docuemnts containing 50000 words , we can create 10 topics where each topic has 100 documents.\n",
        "#topic modelling is useful to identify and categorize the documents .\n",
        "#it is done by LDA\n",
        "#LatentDirichletAllocation - Latent means -about to or in future topic"
      ],
      "metadata": {
        "id": "rFBMZfFfh9PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-jOUX2zLBFO"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "data =['']\n",
        "\n",
        "# define the number of topics\n",
        "n_topics = 5\n",
        "\n",
        "# create a Latent Dirichlet Allocation model\n",
        "lda = LatentDirichletAllocation(n_components=n_topics)\n",
        "\n",
        "# fit the model to the data\n",
        "lda.fit(data)\n",
        "\n",
        "# transform the data using the fitted model\n",
        "transformed = lda.transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "FtQl8tJYj7-J",
        "outputId": "b37b4663-0430-4599-abf8-226ecc71e1ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  year                                              title  \\\n",
              "0        1  1987  Self-Organization of Associative Database and ...   \n",
              "1       10  1987  A Mean Field Theory of Layer IV of Visual Cort...   \n",
              "2      100  1988  Storing Covariance by the Associative Long-Ter...   \n",
              "3     1000  1994  Bayesian Query Construction for Neural Network...   \n",
              "4     1001  1994  Neural Network Ensembles, Cross Validation, an...   \n",
              "...    ...   ...                                                ...   \n",
              "7236   994  1994                Single Transistor Learning Synapses   \n",
              "7237   996  1994  Bias, Variance and the Combination of Least Sq...   \n",
              "7238   997  1994          A Real Time Clustering CMOS Neural Engine   \n",
              "7239   998  1994  Learning direction in global motion: two class...   \n",
              "7240   999  1994  Correlation and Interpolation Networks for Rea...   \n",
              "\n",
              "     event_type                                           pdf_name  \\\n",
              "0           NaN  1-self-organization-of-associative-database-an...   \n",
              "1           NaN  10-a-mean-field-theory-of-layer-iv-of-visual-c...   \n",
              "2           NaN  100-storing-covariance-by-the-associative-long...   \n",
              "3           NaN  1000-bayesian-query-construction-for-neural-ne...   \n",
              "4           NaN  1001-neural-network-ensembles-cross-validation...   \n",
              "...         ...                                                ...   \n",
              "7236        NaN        994-single-transistor-learning-synapses.pdf   \n",
              "7237        NaN  996-bias-variance-and-the-combination-of-least...   \n",
              "7238        NaN  997-a-real-time-clustering-cmos-neural-engine.pdf   \n",
              "7239        NaN  998-learning-direction-in-global-motion-two-cl...   \n",
              "7240        NaN  999-correlation-and-interpolation-networks-for...   \n",
              "\n",
              "              abstract                                         paper_text  \n",
              "0     Abstract Missing  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1     Abstract Missing  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2     Abstract Missing  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3     Abstract Missing  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4     Abstract Missing  Neural Network Ensembles, Cross\\nValidation, a...  \n",
              "...                ...                                                ...  \n",
              "7236  Abstract Missing  Single Transistor Learning Synapses\\n\\nPaul Ha...  \n",
              "7237  Abstract Missing  Bias, Variance and the Combination of\\nLeast S...  \n",
              "7238  Abstract Missing  A Real Time Clustering CMOS\\nNeural Engine\\nT....  \n",
              "7239  Abstract Missing  Learning direction in global motion: two\\nclas...  \n",
              "7240  Abstract Missing  Correlation and Interpolation Networks for\\nRe...  \n",
              "\n",
              "[7241 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-799f9f5c-1fd7-4c40-9672-3aecb5bde15c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7236</th>\n",
              "      <td>994</td>\n",
              "      <td>1994</td>\n",
              "      <td>Single Transistor Learning Synapses</td>\n",
              "      <td>NaN</td>\n",
              "      <td>994-single-transistor-learning-synapses.pdf</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Single Transistor Learning Synapses\\n\\nPaul Ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7237</th>\n",
              "      <td>996</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bias, Variance and the Combination of Least Sq...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>996-bias-variance-and-the-combination-of-least...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bias, Variance and the Combination of\\nLeast S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7238</th>\n",
              "      <td>997</td>\n",
              "      <td>1994</td>\n",
              "      <td>A Real Time Clustering CMOS Neural Engine</td>\n",
              "      <td>NaN</td>\n",
              "      <td>997-a-real-time-clustering-cmos-neural-engine.pdf</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>A Real Time Clustering CMOS\\nNeural Engine\\nT....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7239</th>\n",
              "      <td>998</td>\n",
              "      <td>1994</td>\n",
              "      <td>Learning direction in global motion: two class...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>998-learning-direction-in-global-motion-two-cl...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Learning direction in global motion: two\\nclas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7240</th>\n",
              "      <td>999</td>\n",
              "      <td>1994</td>\n",
              "      <td>Correlation and Interpolation Networks for Rea...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>999-correlation-and-interpolation-networks-for...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Correlation and Interpolation Networks for\\nRe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7241 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-799f9f5c-1fd7-4c40-9672-3aecb5bde15c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-799f9f5c-1fd7-4c40-9672-3aecb5bde15c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-799f9f5c-1fd7-4c40-9672-3aecb5bde15c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-362808d8-aca2-4aaf-959d-6fb01449f060\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-362808d8-aca2-4aaf-959d-6fb01449f060')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-362808d8-aca2-4aaf-959d-6fb01449f060 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#read the data\n",
        "# NIPS research papers\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "papers = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/papers.csv\")\n",
        "#dataset = pd.read_csv('tweet_emotions.csv')\n",
        "papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "nnbYf0E3kJ2H",
        "outputId": "e9a10bbd-b82f-489f-c970-b3563ea78d46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      year                                              title  \\\n",
              "4221  2012     Active Learning of Multi-Index Function Models   \n",
              "2135  2005            Learning Cue-Invariant Visual Responses   \n",
              "6682  2017  Discovering Potential Correlations via Hyperco...   \n",
              "3216  2010  A Primal-Dual Message-Passing Algorithm for Ap...   \n",
              "4251  2012  Learning Image Descriptors with the Boosting-T...   \n",
              "\n",
              "                                               abstract  \\\n",
              "4221  We consider the problem of actively learning \\...   \n",
              "2135                                   Abstract Missing   \n",
              "6682  Discovering a correlation from one variable to...   \n",
              "3216  In this paper we propose an approximated learn...   \n",
              "4251  In this paper we  apply   boosting  to  learn ...   \n",
              "\n",
              "                                             paper_text  \n",
              "4221  Active Learning of Multi-Index Function Models...  \n",
              "2135  Learning Cue-Invariant Visual Responses\\nJarmo...  \n",
              "6682  Discovering Potential Correlations via\\nHyperc...  \n",
              "3216  A Primal-Dual Message-Passing Algorithm for\\nA...  \n",
              "4251  Learning Image Descriptors with the Boosting-T...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0cb24454-37dc-4f27-9624-5afbe7c4a331\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4221</th>\n",
              "      <td>2012</td>\n",
              "      <td>Active Learning of Multi-Index Function Models</td>\n",
              "      <td>We consider the problem of actively learning \\...</td>\n",
              "      <td>Active Learning of Multi-Index Function Models...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2135</th>\n",
              "      <td>2005</td>\n",
              "      <td>Learning Cue-Invariant Visual Responses</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Learning Cue-Invariant Visual Responses\\nJarmo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6682</th>\n",
              "      <td>2017</td>\n",
              "      <td>Discovering Potential Correlations via Hyperco...</td>\n",
              "      <td>Discovering a correlation from one variable to...</td>\n",
              "      <td>Discovering Potential Correlations via\\nHyperc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3216</th>\n",
              "      <td>2010</td>\n",
              "      <td>A Primal-Dual Message-Passing Algorithm for Ap...</td>\n",
              "      <td>In this paper we propose an approximated learn...</td>\n",
              "      <td>A Primal-Dual Message-Passing Algorithm for\\nA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4251</th>\n",
              "      <td>2012</td>\n",
              "      <td>Learning Image Descriptors with the Boosting-T...</td>\n",
              "      <td>In this paper we  apply   boosting  to  learn ...</td>\n",
              "      <td>Learning Image Descriptors with the Boosting-T...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cb24454-37dc-4f27-9624-5afbe7c4a331')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0cb24454-37dc-4f27-9624-5afbe7c4a331 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0cb24454-37dc-4f27-9624-5afbe7c4a331');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a183e069-2518-445f-94a9-e67601163da1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a183e069-2518-445f-94a9-e67601163da1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a183e069-2518-445f-94a9-e67601163da1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#taking a sample of 100 documents\n",
        "papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1).sample(100)\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx9CMMhT1NZS",
        "outputId": "11d02d36-a0be-4905-f988-f6a4726ac4db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "papers.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w91PmUZDv1Fj"
      },
      "outputs": [],
      "source": [
        "papers.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKul_WVvsjEr",
        "outputId": "09b2690d-e1ee-4af9-b9a1-846e8c4bbb71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    active learning of multi-index function models...\n",
              "1    learning cue-invariant visual responses\\njarmo...\n",
              "2    discovering potential correlations via\\nhyperc...\n",
              "3    a primal-dual message-passing algorithm for\\na...\n",
              "4    learning image descriptors with the boosting-t...\n",
              "Name: paper_text_processed, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#Test preprocessing\n",
        "# Load the regular expression library\n",
        "import re\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pe0OKsbvyiN",
        "outputId": "e43df183-ada1-4a82-d117-42a6b077478d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-ff73ad5e2859>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  papers['paper_text_processed'][i] = simple_preprocess(papers['paper_text_processed'][i])\n"
          ]
        }
      ],
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "for i in range(len(papers['paper_text_processed'])):\n",
        "  papers['paper_text_processed'][i] = simple_preprocess(papers['paper_text_processed'][i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGPf6DXE5GpV",
        "outputId": "020422cb-739d-40b7-d968-6a590a940713"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3488"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(papers['paper_text_processed'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tJKQ62b6MRM",
        "outputId": "823baf68-4179-4203-bb86-c10bb5911bf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tensor',\n",
              " 'decomposition',\n",
              " 'for',\n",
              " 'fast',\n",
              " 'parsing',\n",
              " 'with',\n",
              " 'latent',\n",
              " 'variable',\n",
              " 'pcfgs',\n",
              " 'shay',\n",
              " 'cohen',\n",
              " 'and',\n",
              " 'michael',\n",
              " 'collins',\n",
              " 'department',\n",
              " 'of',\n",
              " 'computer',\n",
              " 'science',\n",
              " 'columbia',\n",
              " 'university',\n",
              " 'new',\n",
              " 'york',\n",
              " 'ny',\n",
              " 'scohenmcollins',\n",
              " 'cscolumbiaedu',\n",
              " 'abstract',\n",
              " 'we',\n",
              " 'describe',\n",
              " 'an',\n",
              " 'approach',\n",
              " 'to',\n",
              " 'speed',\n",
              " 'up',\n",
              " 'inference',\n",
              " 'with',\n",
              " 'latent',\n",
              " 'variable',\n",
              " 'pcfgs',\n",
              " 'which',\n",
              " 'have',\n",
              " 'been',\n",
              " 'shown',\n",
              " 'to',\n",
              " 'be',\n",
              " 'highly',\n",
              " 'effective',\n",
              " 'for',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'parsing',\n",
              " 'our',\n",
              " 'approach',\n",
              " 'is',\n",
              " 'based',\n",
              " 'on',\n",
              " 'tensor',\n",
              " 'formulation',\n",
              " 'recently',\n",
              " 'introduced',\n",
              " 'for',\n",
              " 'spectral',\n",
              " 'estimation',\n",
              " 'of',\n",
              " 'latent',\n",
              " 'variable',\n",
              " 'pcfgs',\n",
              " 'coupled',\n",
              " 'with',\n",
              " 'tensor',\n",
              " 'decomposition',\n",
              " 'algorithm',\n",
              " 'well',\n",
              " 'known',\n",
              " 'in',\n",
              " 'the',\n",
              " 'multilinear',\n",
              " 'algebra',\n",
              " 'literature',\n",
              " 'we',\n",
              " 'also',\n",
              " 'describe',\n",
              " 'an',\n",
              " 'error',\n",
              " 'bound',\n",
              " 'for',\n",
              " 'this',\n",
              " 'approximation',\n",
              " 'which',\n",
              " 'gives',\n",
              " 'guarantees',\n",
              " 'showing',\n",
              " 'that',\n",
              " 'if',\n",
              " 'the',\n",
              " 'underlying',\n",
              " 'tensors',\n",
              " 'are',\n",
              " 'well',\n",
              " 'approximated',\n",
              " 'then',\n",
              " 'the',\n",
              " 'probability',\n",
              " 'distribution',\n",
              " 'over',\n",
              " 'trees',\n",
              " 'will',\n",
              " 'also',\n",
              " 'be',\n",
              " 'well',\n",
              " 'approximated',\n",
              " 'empirical',\n",
              " 'evaluation',\n",
              " 'on',\n",
              " 'real',\n",
              " 'world',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'parsing',\n",
              " 'data',\n",
              " 'demonstrates',\n",
              " 'significant',\n",
              " 'speed',\n",
              " 'up',\n",
              " 'at',\n",
              " 'minimal',\n",
              " 'cost',\n",
              " 'for',\n",
              " 'parsing',\n",
              " 'performance',\n",
              " 'introduction',\n",
              " 'latent',\n",
              " 'variable',\n",
              " 'models',\n",
              " 'have',\n",
              " 'shown',\n",
              " 'great',\n",
              " 'success',\n",
              " 'in',\n",
              " 'various',\n",
              " 'fields',\n",
              " 'including',\n",
              " 'computational',\n",
              " 'linguistics',\n",
              " 'and',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'in',\n",
              " 'computational',\n",
              " 'linguistics',\n",
              " 'for',\n",
              " 'example',\n",
              " 'latent',\n",
              " 'variable',\n",
              " 'models',\n",
              " 'are',\n",
              " 'widely',\n",
              " 'used',\n",
              " 'for',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'parsing',\n",
              " 'using',\n",
              " 'models',\n",
              " 'called',\n",
              " 'latent',\n",
              " 'variable',\n",
              " 'pcfgs',\n",
              " 'pcfgs',\n",
              " 'the',\n",
              " 'mainstay',\n",
              " 'for',\n",
              " 'estimation',\n",
              " 'of',\n",
              " 'pcfgs',\n",
              " 'has',\n",
              " 'been',\n",
              " 'the',\n",
              " 'expectation',\n",
              " 'maximization',\n",
              " 'algorithm',\n",
              " 'though',\n",
              " 'other',\n",
              " 'algorithms',\n",
              " 'such',\n",
              " 'as',\n",
              " 'spectral',\n",
              " 'algorithms',\n",
              " 'have',\n",
              " 'been',\n",
              " 'devised',\n",
              " 'by',\n",
              " 'product',\n",
              " 'of',\n",
              " 'the',\n",
              " 'spectral',\n",
              " 'algorithm',\n",
              " 'presented',\n",
              " 'in',\n",
              " 'is',\n",
              " 'tensor',\n",
              " 'formulation',\n",
              " 'for',\n",
              " 'computing',\n",
              " 'the',\n",
              " 'inside',\n",
              " 'outside',\n",
              " 'probabilities',\n",
              " 'of',\n",
              " 'pcfg',\n",
              " 'tensor',\n",
              " 'products',\n",
              " 'or',\n",
              " 'matrix',\n",
              " 'vector',\n",
              " 'products',\n",
              " 'in',\n",
              " 'certain',\n",
              " 'cases',\n",
              " 'are',\n",
              " 'used',\n",
              " 'as',\n",
              " 'the',\n",
              " 'basic',\n",
              " 'operation',\n",
              " 'for',\n",
              " 'marginalization',\n",
              " 'over',\n",
              " 'the',\n",
              " 'latent',\n",
              " 'annotations',\n",
              " 'of',\n",
              " 'the',\n",
              " 'pcfg',\n",
              " 'the',\n",
              " 'computational',\n",
              " 'complexity',\n",
              " 'with',\n",
              " 'the',\n",
              " 'tensor',\n",
              " 'formulation',\n",
              " 'or',\n",
              " 'with',\n",
              " 'plain',\n",
              " 'cky',\n",
              " 'for',\n",
              " 'that',\n",
              " 'matter',\n",
              " 'is',\n",
              " 'cubic',\n",
              " 'in',\n",
              " 'the',\n",
              " 'number',\n",
              " 'of',\n",
              " 'latent',\n",
              " 'states',\n",
              " 'in',\n",
              " 'the',\n",
              " 'pcfg',\n",
              " 'this',\n",
              " 'multiplicative',\n",
              " 'factor',\n",
              " 'can',\n",
              " 'be',\n",
              " 'prohibitive',\n",
              " 'for',\n",
              " 'large',\n",
              " 'number',\n",
              " 'of',\n",
              " 'hidden',\n",
              " 'states',\n",
              " 'various',\n",
              " 'heuristics',\n",
              " 'are',\n",
              " 'used',\n",
              " 'in',\n",
              " 'practice',\n",
              " 'to',\n",
              " 'avoid',\n",
              " 'this',\n",
              " 'problem',\n",
              " 'in',\n",
              " 'this',\n",
              " 'paper',\n",
              " 'we',\n",
              " 'show',\n",
              " 'that',\n",
              " 'tensor',\n",
              " 'decomposition',\n",
              " 'can',\n",
              " 'be',\n",
              " 'used',\n",
              " 'to',\n",
              " 'significantly',\n",
              " 'speed',\n",
              " 'up',\n",
              " 'the',\n",
              " 'parsing',\n",
              " 'performance',\n",
              " 'with',\n",
              " 'pcfgs',\n",
              " 'our',\n",
              " 'approach',\n",
              " 'is',\n",
              " 'also',\n",
              " 'provided',\n",
              " 'with',\n",
              " 'theoretical',\n",
              " 'guarantee',\n",
              " 'given',\n",
              " 'the',\n",
              " 'accuracy',\n",
              " 'of',\n",
              " 'the',\n",
              " 'tensor',\n",
              " 'decomposition',\n",
              " 'one',\n",
              " 'can',\n",
              " 'compute',\n",
              " 'how',\n",
              " 'accurate',\n",
              " 'the',\n",
              " 'approximate',\n",
              " 'parser',\n",
              " 'is',\n",
              " 'the',\n",
              " 'rest',\n",
              " 'of',\n",
              " 'this',\n",
              " 'paper',\n",
              " 'is',\n",
              " 'organized',\n",
              " 'as',\n",
              " 'follows',\n",
              " 'we',\n",
              " 'give',\n",
              " 'notation',\n",
              " 'and',\n",
              " 'background',\n",
              " 'in',\n",
              " 'and',\n",
              " 'then',\n",
              " 'present',\n",
              " 'the',\n",
              " 'main',\n",
              " 'approach',\n",
              " 'in',\n",
              " 'we',\n",
              " 'describe',\n",
              " 'experimental',\n",
              " 'results',\n",
              " 'in',\n",
              " 'and',\n",
              " 'conclude',\n",
              " 'in',\n",
              " 'notation',\n",
              " 'given',\n",
              " 'matrix',\n",
              " 'or',\n",
              " 'vector',\n",
              " 'we',\n",
              " 'write',\n",
              " 'or',\n",
              " 'for',\n",
              " 'the',\n",
              " 'associated',\n",
              " 'transpose',\n",
              " 'for',\n",
              " 'any',\n",
              " 'integer',\n",
              " 'we',\n",
              " 'use',\n",
              " 'to',\n",
              " 'denote',\n",
              " 'the',\n",
              " 'set',\n",
              " 'we',\n",
              " 'will',\n",
              " 'make',\n",
              " 'use',\n",
              " 'of',\n",
              " 'tensors',\n",
              " 'of',\n",
              " 'rank',\n",
              " 'all',\n",
              " 'pcfgs',\n",
              " 'in',\n",
              " 'this',\n",
              " 'paper',\n",
              " 'are',\n",
              " 'assumed',\n",
              " 'to',\n",
              " 'be',\n",
              " 'in',\n",
              " 'chomsky',\n",
              " 'normal',\n",
              " 'form',\n",
              " 'our',\n",
              " 'approach',\n",
              " 'generalizes',\n",
              " 'to',\n",
              " 'arbitrary',\n",
              " 'pcfgs',\n",
              " 'which',\n",
              " 'require',\n",
              " 'tensors',\n",
              " 'of',\n",
              " 'higher',\n",
              " 'rank',\n",
              " 'definition',\n",
              " 'tensor',\n",
              " 'mmm',\n",
              " 'is',\n",
              " 'set',\n",
              " 'of',\n",
              " 'parameters',\n",
              " 'cijk',\n",
              " 'for',\n",
              " 'given',\n",
              " 'tensor',\n",
              " 'and',\n",
              " 'vectors',\n",
              " 'rm',\n",
              " 'and',\n",
              " 'yp',\n",
              " 'rm',\n",
              " 'we',\n",
              " 'define',\n",
              " 'to',\n",
              " 'be',\n",
              " 'the',\n",
              " 'dimensional',\n",
              " 'row',\n",
              " 'vector',\n",
              " 'with',\n",
              " 'components',\n",
              " 'cijk',\n",
              " 'yj',\n",
              " 'yk',\n",
              " 'hence',\n",
              " 'can',\n",
              " 'be',\n",
              " 'interpreted',\n",
              " 'as',\n",
              " 'function',\n",
              " 'rm',\n",
              " 'rm',\n",
              " 'that',\n",
              " 'maps',\n",
              " 'vectors',\n",
              " 'and',\n",
              " 'to',\n",
              " 'row',\n",
              " 'vector',\n",
              " 'in',\n",
              " 'addition',\n",
              " 'we',\n",
              " 'define',\n",
              " 'the',\n",
              " 'tensor',\n",
              " 'mmm',\n",
              " 'for',\n",
              " 'any',\n",
              " 'tensor',\n",
              " 'mmm',\n",
              " 'to',\n",
              " 'be',\n",
              " 'the',\n",
              " 'function',\n",
              " 'rm',\n",
              " 'rm',\n",
              " 'rm',\n",
              " 'defined',\n",
              " 'as',\n",
              " 'cijk',\n",
              " 'yi',\n",
              " 'yj',\n",
              " 'similarly',\n",
              " 'for',\n",
              " 'any',\n",
              " 'tensor',\n",
              " 'we',\n",
              " 'define',\n",
              " 'rm',\n",
              " 'rm',\n",
              " 'rm',\n",
              " 'as',\n",
              " 'cijk',\n",
              " 'yi',\n",
              " 'yk',\n",
              " 'note',\n",
              " 'that',\n",
              " 'and',\n",
              " 'are',\n",
              " 'both',\n",
              " 'column',\n",
              " 'vectors',\n",
              " 'for',\n",
              " 'two',\n",
              " 'vectors',\n",
              " 'rm',\n",
              " 'and',\n",
              " 'rm',\n",
              " 'we',\n",
              " 'denote',\n",
              " 'by',\n",
              " 'rm',\n",
              " 'the',\n",
              " 'hadamard',\n",
              " 'product',\n",
              " 'of',\n",
              " 'and',\n",
              " 'ie',\n",
              " 'xi',\n",
              " 'yi',\n",
              " 'finally',\n",
              " 'for',\n",
              " 'vectors',\n",
              " 'rm',\n",
              " 'xy',\n",
              " 'is',\n",
              " 'the',\n",
              " 'tensor',\n",
              " 'rmmm',\n",
              " 'where',\n",
              " 'dijk',\n",
              " 'xi',\n",
              " 'yj',\n",
              " 'zk',\n",
              " 'this',\n",
              " 'is',\n",
              " 'analogous',\n",
              " 'to',\n",
              " 'the',\n",
              " 'outer',\n",
              " 'product',\n",
              " 'xy',\n",
              " 'ij',\n",
              " 'xi',\n",
              " 'yj',\n",
              " 'latent',\n",
              " 'variable',\n",
              " 'parsing',\n",
              " 'in',\n",
              " 'this',\n",
              " 'section',\n",
              " 'we',\n",
              " 'describe',\n",
              " 'latent',\n",
              " 'variable',\n",
              " 'pcfgs',\n",
              " 'and',\n",
              " 'their',\n",
              " 'parsing',\n",
              " 'algorithms',\n",
              " 'latent',\n",
              " 'variable',\n",
              " 'pcfgs',\n",
              " 'this',\n",
              " 'section',\n",
              " 'gives',\n",
              " 'definition',\n",
              " 'of',\n",
              " 'the',\n",
              " 'pcfg',\n",
              " 'formalism',\n",
              " 'used',\n",
              " 'in',\n",
              " 'this',\n",
              " 'paper',\n",
              " 'we',\n",
              " 'follow',\n",
              " 'the',\n",
              " 'definitions',\n",
              " 'given',\n",
              " 'in',\n",
              " 'an',\n",
              " 'pcfg',\n",
              " 'is',\n",
              " 'tuple',\n",
              " 'where',\n",
              " 'is',\n",
              " 'the',\n",
              " 'set',\n",
              " 'of',\n",
              " 'non',\n",
              " 'terminal',\n",
              " 'symbols',\n",
              " 'in',\n",
              " 'the',\n",
              " 'grammar',\n",
              " 'is',\n",
              " 'finite',\n",
              " 'set',\n",
              " 'of',\n",
              " 'in',\n",
              " 'terminals',\n",
              " 'is',\n",
              " 'finite',\n",
              " 'set',\n",
              " 'of',\n",
              " 'pre',\n",
              " 'terminals',\n",
              " 'we',\n",
              " 'assume',\n",
              " 'that',\n",
              " 'and',\n",
              " 'hence',\n",
              " 'we',\n",
              " 'have',\n",
              " 'partitioned',\n",
              " 'the',\n",
              " 'set',\n",
              " 'of',\n",
              " 'non',\n",
              " 'terminals',\n",
              " 'into',\n",
              " 'two',\n",
              " 'subsets',\n",
              " 'is',\n",
              " 'the',\n",
              " 'set',\n",
              " 'of',\n",
              " 'possible',\n",
              " 'hidden',\n",
              " 'states',\n",
              " 'is',\n",
              " 'the',\n",
              " 'set',\n",
              " 'of',\n",
              " 'possible',\n",
              " 'words',\n",
              " 'for',\n",
              " 'all',\n",
              " 'we',\n",
              " 'have',\n",
              " 'context',\n",
              " 'free',\n",
              " 'rule',\n",
              " 'for',\n",
              " 'all',\n",
              " 'we',\n",
              " 'have',\n",
              " 'context',\n",
              " 'free',\n",
              " 'rule',\n",
              " 'note',\n",
              " 'that',\n",
              " 'for',\n",
              " 'any',\n",
              " 'binary',\n",
              " 'rule',\n",
              " 'it',\n",
              " 'holds',\n",
              " 'that',\n",
              " 'and',\n",
              " 'for',\n",
              " 'any',\n",
              " 'unary',\n",
              " 'rule',\n",
              " 'it',\n",
              " 'holds',\n",
              " 'that',\n",
              " 'the',\n",
              " 'set',\n",
              " 'of',\n",
              " 'skeletal',\n",
              " 'rules',\n",
              " 'is',\n",
              " 'defined',\n",
              " 'as',\n",
              " 'the',\n",
              " 'parameters',\n",
              " 'of',\n",
              " 'the',\n",
              " 'model',\n",
              " 'are',\n",
              " 'as',\n",
              " 'follows',\n",
              " 'for',\n",
              " 'each',\n",
              " 'and',\n",
              " 'we',\n",
              " 'have',\n",
              " 'parameter',\n",
              " 'for',\n",
              " 'each',\n",
              " 'and',\n",
              " 'we',\n",
              " 'have',\n",
              " 'parameter',\n",
              " 'an',\n",
              " 'pcfg',\n",
              " 'corresponds',\n",
              " 'to',\n",
              " 'regular',\n",
              " 'pcfg',\n",
              " 'with',\n",
              " 'non',\n",
              " 'terminals',\n",
              " 'annotated',\n",
              " 'with',\n",
              " 'latent',\n",
              " 'states',\n",
              " 'for',\n",
              " 'each',\n",
              " 'triplet',\n",
              " 'of',\n",
              " 'latent',\n",
              " 'states',\n",
              " 'and',\n",
              " 'rule',\n",
              " 'we',\n",
              " 'have',\n",
              " 'rule',\n",
              " 'probability',\n",
              " 'similarly',\n",
              " 'we',\n",
              " 'also',\n",
              " 'have',\n",
              " 'parameters',\n",
              " 'in',\n",
              " 'addition',\n",
              " 'there',\n",
              " 'are',\n",
              " 'initial',\n",
              " 'probabilities',\n",
              " 'of',\n",
              " 'generating',\n",
              " 'non',\n",
              " 'terminal',\n",
              " 'with',\n",
              " 'latent',\n",
              " 'at',\n",
              " 'the',\n",
              " 'top',\n",
              " 'of',\n",
              " 'the',\n",
              " 'tree',\n",
              " 'denoted',\n",
              " 'by',\n",
              " 'pcfgs',\n",
              " 'induce',\n",
              " 'distributions',\n",
              " 'over',\n",
              " 'two',\n",
              " 'type',\n",
              " 'of',\n",
              " 'trees',\n",
              " 'skeletal',\n",
              " 'trees',\n",
              " 'ie',\n",
              " 'trees',\n",
              " 'without',\n",
              " 'values',\n",
              " 'for',\n",
              " 'latent',\n",
              " 'states',\n",
              " 'these',\n",
              " 'trees',\n",
              " 'are',\n",
              " 'observed',\n",
              " 'in',\n",
              " 'data',\n",
              " 'and',\n",
              " 'full',\n",
              " 'trees',\n",
              " 'trees',\n",
              " 'with',\n",
              " 'values',\n",
              " 'for',\n",
              " 'latent',\n",
              " 'states',\n",
              " 'skeletal',\n",
              " 'tree',\n",
              " 'consists',\n",
              " 'of',\n",
              " 'sequence',\n",
              " 'of',\n",
              " 'rules',\n",
              " 'rn',\n",
              " 'where',\n",
              " 'ri',\n",
              " 'or',\n",
              " 'ri',\n",
              " 'see',\n",
              " 'figure',\n",
              " 'for',\n",
              " 'an',\n",
              " 'example',\n",
              " 'we',\n",
              " 'now',\n",
              " 'turn',\n",
              " 'to',\n",
              " 'the',\n",
              " 'problem',\n",
              " 'of',\n",
              " 'computing',\n",
              " 'the',\n",
              " 'probability',\n",
              " 'of',\n",
              " 'skeletal',\n",
              " 'tree',\n",
              " 'by',\n",
              " 'marginalizing',\n",
              " 'out',\n",
              " 'the',\n",
              " 'latent',\n",
              " 'states',\n",
              " 'of',\n",
              " 'full',\n",
              " 'trees',\n",
              " 'let',\n",
              " 'rn',\n",
              " 'be',\n",
              " 'derivation',\n",
              " 'and',\n",
              " 'let',\n",
              " 'ai',\n",
              " 'be',\n",
              " 'the',\n",
              " 'non',\n",
              " 'terminal',\n",
              " 'on',\n",
              " 'the',\n",
              " 'left',\n",
              " 'hand',\n",
              " 'side',\n",
              " 'of',\n",
              " 'rule',\n",
              " 'ri',\n",
              " 'for',\n",
              " 'any',\n",
              " 'ri',\n",
              " 'define',\n",
              " 'hi',\n",
              " 'to',\n",
              " 'be',\n",
              " 'the',\n",
              " 'latent',\n",
              " 'state',\n",
              " 'associated',\n",
              " 'with',\n",
              " 'the',\n",
              " 'left',\n",
              " 'child',\n",
              " 'of',\n",
              " 'the',\n",
              " 'rule',\n",
              " 'ri',\n",
              " 'and',\n",
              " 'hi',\n",
              " 'to',\n",
              " 'be',\n",
              " 'the',\n",
              " 'hidden',\n",
              " 'variable',\n",
              " 'value',\n",
              " 'associated',\n",
              " 'with',\n",
              " 'the',\n",
              " 'right',\n",
              " 'child',\n",
              " 'the',\n",
              " 'distribution',\n",
              " 'over',\n",
              " 'full',\n",
              " 'trees',\n",
              " 'is',\n",
              " 'then',\n",
              " 'rn',\n",
              " 'hn',\n",
              " 'ai',\n",
              " 'ri',\n",
              " 'hi',\n",
              " 'hi',\n",
              " 'hi',\n",
              " 'ai',\n",
              " 'ai',\n",
              " 'ri',\n",
              " 'hi',\n",
              " 'ai',\n",
              " 'np',\n",
              " 'vp',\n",
              " 'the',\n",
              " 'man',\n",
              " 'saw',\n",
              " 'him',\n",
              " 'np',\n",
              " 'vp',\n",
              " 'np',\n",
              " 'the',\n",
              " 'man',\n",
              " 'vp',\n",
              " 'saw',\n",
              " 'him',\n",
              " 'figure',\n",
              " 'an',\n",
              " 'tree',\n",
              " 'with',\n",
              " 'its',\n",
              " 'sequence',\n",
              " 'of',\n",
              " 'rules',\n",
              " 'the',\n",
              " 'nodes',\n",
              " 'in',\n",
              " 'the',\n",
              " 'tree',\n",
              " 'are',\n",
              " 'indexed',\n",
              " 'by',\n",
              " 'the',\n",
              " 'derivation',\n",
              " 'order',\n",
              " 'which',\n",
              " 'is',\n",
              " 'canonicalized',\n",
              " 'as',\n",
              " 'top',\n",
              " 'down',\n",
              " 'left',\n",
              " 'most',\n",
              " 'derviation',\n",
              " 'marginalizing',\n",
              " 'out',\n",
              " 'the',\n",
              " 'latent',\n",
              " 'states',\n",
              " 'leads',\n",
              " 'to',\n",
              " 'the',\n",
              " 'distribution',\n",
              " 'over',\n",
              " 'the',\n",
              " 'skeletal',\n",
              " 'tree',\n",
              " 'rn',\n",
              " 'rn',\n",
              " 'hn',\n",
              " 'rn',\n",
              " 'hn',\n",
              " 'it',\n",
              " 'will',\n",
              " 'be',\n",
              " 'important',\n",
              " 'for',\n",
              " 'the',\n",
              " 'rest',\n",
              " 'of',\n",
              " 'this',\n",
              " 'paper',\n",
              " 'to',\n",
              " 'use',\n",
              " 'of',\n",
              " 'matrix',\n",
              " 'form',\n",
              " 'of',\n",
              " 'parameters',\n",
              " 'of',\n",
              " 'an',\n",
              " 'pcfg',\n",
              " 'as',\n",
              " 'follows',\n",
              " 'for',\n",
              " 'each',\n",
              " 'we',\n",
              " 'define',\n",
              " 'ab',\n",
              " 'rmmm',\n",
              " 'to',\n",
              " 'be',\n",
              " 'the',\n",
              " 'tensor',\n",
              " 'with',\n",
              " 'values',\n",
              " 'thab',\n",
              " 'for',\n",
              " 'each',\n",
              " 'we',\n",
              " 'define',\n",
              " 'qax',\n",
              " 'to',\n",
              " 'be',\n",
              " 'the',\n",
              " 'vector',\n",
              " 'with',\n",
              " 'values',\n",
              " 'for',\n",
              " 'for',\n",
              " 'each',\n",
              " 'we',\n",
              " 'define',\n",
              " 'the',\n",
              " 'vector',\n",
              " 'rm',\n",
              " 'where',\n",
              " 'parameter',\n",
              " 'estimation',\n",
              " 'several',\n",
              " 'ways',\n",
              " ...]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "papers['paper_text_processed'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht5F1BuFsosd",
        "outputId": "0f3f713a-7e67-4dcd-8e99-98c427766f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['active', 'learning', 'multi', 'index', 'function', 'models', 'hemant', 'tyagi', 'volkan', 'cevher', 'lions', 'epfl', 'abstract', 'consider', 'problem', 'actively', 'learning', 'multi', 'index', 'functions', 'form', 'pk', 'ax', 'gi', 'ati', 'point', 'evaluations', 'assume', 'function', 'defined']\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc))\n",
        "             if word not in stop_words] for doc in texts]\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "# remove stop words\n",
        "data_words = remove_stopwords(data_words)\n",
        "print(data_words[:1][0][:30])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "Jfzgjec1wfST",
        "outputId": "7e0d37b5-a0db-4928-dd5b-55e8f0b72ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'learning image descriptor boosting trick tomasz trzcinski mario christoudias vincent lepetit pascal fua cvlab epfl lausanne switzerland firstnamelastname epflch abstract paper apply boosting learn complex non linear local visual feature representation drawing inspiration successful application visual object detection main goal local feature descriptor distinctively represent salient image region remaining invariant viewpoint illumination change representation improved using machine learning however past approach mostly limited learning linear feature mapping either original input kernelized input feature space kernelized method proven somewhat effective learning non linear local feature descriptor rely heavily choice appropriate kernel function whose selection often difficult non intuitive propose use boosting trick obtain non linear mapping input high dimensional feature space non linear feature mapping obtained boosting trick highly intuitive employ gradient based weak learner resulting learned descriptor closely resembles well known sift demonstrated experiment resulting descriptor learned directly intensity patch achieving state art performance introduction representing salient image region way invariant unwanted image transformation crucial computer vision task well known local feature descriptor scale invariant feature transform sift speeded robust feature surf address problem using set hand crafted filter non linear operation descriptor become prevalent even though truly invariant respect various viewpoint illumination change limit applicability effort address limitation fair amount work focused learning local feature descriptor leverage labeled training image patch learn invariant feature representation based local image statistic although significant progress made approach either built top hand crafted representation still require significant parameter tuning relies non analytical objective difficult optimize learning invariant feature representation strongly related learning appropriate similarity measure metric intensity patch invariant unwanted image transformation work descriptor learning predominantly focused area method metric learning applied image data largely focused learning linear feature mapping either original input kernelized input feature space includes previous boosting based metric learning method thus far limited learning linear feature transformation way non linearity modeled using predefined similarity kernel function implicitly map input feature high dimensional feature space transformation assumed linear method proven somewhat effective learning non linear local feature mapping choosing appropriate kernel function often nonintuitive remains challenging largely open problem additionally kernel method involve optimization whose problem complexity grows quadratically number training example making difficult apply large problem typical local descriptor learning paper apply boosting learn complex non linear local visual feature representation drawing inspiration successful application visual object detection image patch appearance modeled using local non linear filter evaluated within image patch effectively selected boosting analogous kernel trick approach seen applying boosting trick obtain non linear mapping input high dimensional feature space unlike kernel method boosting trick allows definition intuitive non linear feature mapping also learning approach scale linearly number training example making easily amenable large scale problem result highly accurate descriptor matching build upon also relies boosting compute descriptor show use way efficiently select feature compute compact representation also replace simple weak learner non linear filter adapted problem particular employ image gradient based weak learner similar share close connection non linear filter used proven image descriptor sift histogram oriented gradient hog approach seen generalization method cast within principled learning framework seen experiment descriptor learned directly intensity patch result state art performance rivaling hand designed equivalent evaluate approach consider image patch dataset containing several hundred thousand image patch varying viewpoint illumination condition baseline compare leading contemporary hand designed learned local feature descriptor demonstrate effectiveness approach challenging dataset significantly outperforming baseline method related work machine learning applied improve matching efficiency accuracy image descriptor feature hashing method improve storage computational requirement image based feature salakhutdinov hinton develop semantic hashing approach based restricted boltzman machine rbms applied binary image digit similarly wei et al present spectral hashing approach learns compact binary code efficient image indexing matching kulis darrell extend idea explicitly minimize error original euclidean computed hamming distance many approach presume given distance similarity measure pre defined input feature space although result efficient description indexing many case limited matching accuracy original input space contrast approach learns nonlinear feature mapping specifically optimized result highly accurate descriptor matching method metric learning learn feature space tailored particular matching task method assume presence annotated label pair triplet encode desired proximity relationship learned feature embedding jain et al learn mahalanobis distance metric defined using either original input kernelized input feature space applied image classification matching alternatively strecha et al employ linear discriminant analysis learn linear feature mapping binary labeled example pair method closely related offering different optimization strategy learning mahalanobis based distance metric method improve matching accuracy learned feature space require presence pre selected kernel function encode non linearity approach well suited certain image indexing classification task task specific kernel function proposed eg however le applicable local image feature matching appropriate choice kernel function le understood boosting also applied learning mahalanobis based distance metric involving highdimensional input space overcoming large computational complexity conventional positive semi definite psd solver based interior point method shen et al proposed psd solver using column generation technique based adaboost later extended involve closed form iterative update recently bi et al devised similar method exhibiting even improvement computational complexity application bio medical imagery method also use boosting learn feature mapping emphasized computational efficiency considering linear feature embeddings approach exhibit similar computational advantage however ability learn non linear feature mapping beyond method proposed similar work brown et al also consider different feature pooling selection strategy gradient based feature resulting descriptor short discriminant however optimize combination handcrafted block parameter criterion considerthe area roc curveis analytical thus difficult optimize generalize well contrast provide generic learning framework finding representation moreover form descriptor much simpler simultaneous work similar idea explored approach assume sub sampled course set pooling region mitigate tractability allow discovery generic pooling configuration boosting work boosted feature learning traced back work dollar et al apply boosting across range different feature pedestrian detection approach probably similar boosted similarity sensitive coding ssc method shakhnarovich learns boosted similarity function family weak learner method later extended used hamming distance linear projection based weak learner considered also boosted ssc often yield fairly high dimensional embeddings approach seen extension boosted ssc form low dimensional feature mapping also show image gradient based weak learner well adapted problem seen experiment approach significantly outperforms boosted ssc applied image intensity patch method given image intensity patch x rd look descriptor x non linear mapping h x space spanned hi collection thresholded non linear response function hi x rd number response function generally large possibly infinite mapping learned minimizing exponential loss respect desired similarity function f x defined image patch pair n x l exp li f xi yi xi yi rd training intensity patch li label indicating whether similar dissimilar pair boosted ssc method proposed considers similarity function defined simply weighted sum thresholded response function f x x hi x hi defines weighted hash function importance dimension given substituting expression equation give n x x lssc exp li j hj xi hj yi j practice large general number possible hi infinite making explicit optimization lssc difficult constitutes problem boosting particularly well suited although boosting greedy optimization scheme provably effective method constructing highly accurate predictor collection weak predictor hi similar kernel trick resulting boosting trick also map observation highdimensional feature space however computes explicit mapping define f x assumed sparse fact rosset et al shown certain setting boosting interpreted imposing l sparsity constraint response function weight seen unlike kernel trick allows definition high dimensional embeddings well suited descriptor matching task whose feature intuitive explanation boosted ssc employ linear response weak predictor based linear projection input contrast consider non linear response function suitable descriptor matching task discussed section addition greedy optimization often yield embeddings although accurate fairly redundant inefficient follows present approach learning compact boosted feature descriptor called low dimensional boosted gradient map l bgm first present modified similarity function well suited learning low dimensional discriminative embeddings boosting next show factorize learned embedding form compact feature descriptor finally gradient based weak learner utilized approach detailed similarity measure mitigate potentially redundant embeddings found boosting propose alternative similarity function model correlation weak response function x flbgm x ij hi x hj h x ah ij h x h x hm x matrix coefficient ij similarity measure generalization equation particular flbgm equivalent boosted ssc similarity measure restricted case diagonal substituting expression equation give n x x llbgm exp lk ij hi xk hj yk ij k although shown llbgm jointly optimized hi using boosting involves fairly complex procedure instead propose two step learning strategy whereby first apply adaboost find hi shown experiment provides effective way select relevant hi apply stochastic gradient descent find optimal weighting selected feature minimizes llbgm formally let p number relevant response function found adaboost p define ap rp p sub matrix corresponding non zero entry explicitly optimized approach note loss function convex ap found optimally respect selected hi addition constrain ij ji optimization restricting solution set symmetric p p matrix yielding symmetric similarity measure flbgm also experimented restrictive form regularization eg constraining ap possitive semi definite however costly gave similar result use simple implementation stochastic gradient descent constant valued step size initialized using diagonal matrix found boosted ssc iterate convergence maximum number iteration reached note weak learner binary precompute exponential term involved derivative data sample constant respect ap significantly speed optimization process embedding factorization similarity function equation defines implicit feature mapping example pair show ap matrix flbgm factorized result compact feature descriptor computed independently input assuming ap symmetric p p matrix factorized following form ap bwbt x k wk bk btk weighting r e r e e e e r e e e r image gradient r keypoint descriptor figure specialized configuration weak response function corresponding regular gridding within image patch addition assuming gaussian weighting result descriptor closely resembles sift one many solution afforded learning framework w diag w wd wk b b bd b rp p equation expressed flbgm x x wk k p x p x bkj hj bki hi x j factorization defines signed inner product embedded feature vector provides increased efficiency respect original similarity measure p ie effective rank ap p factorization represents smoothed version ap discarding lowenergy dimension typically correlate noise leading performance improvement final embedding found approach therefore hlbgm x bt h x hlbgm x r r projection matrix b defines discriminative dimensionality reduction optimized respect exponential loss objective equation seen experiment case redundant hi result considerable feature compression also offering compact description original input patch weak learner boosting trick allows variety non linear embeddings parameterized chosen weak learner family employ gradient based response function form feature descriptor usefulness feature demonstrated visual object detection follows extend feature descriptor matching task illustrating close connection well known sift descriptor following notation weak learner defined x h x r e otherwise x x e x x ek x ek mr mr region e x gradient energy along orientation e location within x r defining rectangular extent within patch gradient energy computed based dot product e gradient orientation pixel orientation e range quantized take value q q q q q number matching two set descriptor size n n p original measure n p n provided factorization resulting significant saving reasonably sized n p p b c figure learned spatial weighting obtained boosted gradient map bgm trained liberty b notre dame c yosemite datasets learned weighting closely resembles gaussian weighting employed sift white circle indicate used sift quantization bin noted representation computed efficiently using integral image non linear gradient response function along thresholding define parameterization weak learner family optimized approach consider specialized configuration illustrated figure corresponds selection weak learner whose r e value parameterized lie along regular grid equally sampling edge orientation within grid cell addition assume gaussian weighting centered patch resulting descriptor closely resembles sift fact configuration weighting corresponds one many solution afforded approach note importance allowing alternative pooling feature selection strategy effectively optimized within framework seen experiment result significant performance gain hand designed sift result section first present overview evaluation framework show result obtained using boosted ssc combined gradient based weak learner described sec continue result generated applying factorized embedding matrix finally present comparison final descriptor state art evaluation framework evaluate performance method using three publicly available datasets liberty notre dame yosemite contain k scale rotation normalized patch patch sampled around interest point detected using difference gaussians correspondence patch found using multi view stereo algorithm datasets created way exhibit substantial perspective distortion various lighting condition ground truth available datasets describes k k k pair patch correspond match pair non match pair evaluation separately consider dataset training use held datasets testing report result evaluation term roc curve error rate done boosted gradient map show performance boost get using gradient based weak learner boosting scheme plot result original boosted ssc method relies thresholded pixel intensity weak learner method us gradient based weak learner instead referred boosted gradient map bgm q quantized orientation bin used throughout experiment see fig dimensional boosted ssc descriptor easily outperformed dimensional bgm descriptor comparing descriptor dimensionality improvement measured term error rate reach furthermore worth noticing dimension bgm performs similarly sift increase dimensionality outperforms sift term error rate comparing dimensional sift obtained increasing granularity orientation bin dimensional bgm extended sift descriptor performs much worse sift additionally normalizes descriptor unit norm however underlying representation otherwise quite similar train liberty k test notre dame k true positive rate true positive rate train liberty k test notre dame k sift boosted ssc bgm bgm bgm bgm bgm sift boosted ssc bgm pca l bgm diag l bgm l bgm l bgm l bgm l bgm false positive rate false positive rate b figure boosted scc using thresholded pixel intensity comparison boosted gradient map bgm approach b result optimization correlation matrix performance evaluated respect factorization dimensionality parenthesis number dimension error rate error rate v bgm indicates boosting similar number non linear classifier add performance prof well tuned sift descriptor visualization learned weighting obtained bgm trained liberty notre dame yosemite datasets displayed figure plot visualization sum across orientation within rectangular region corresponding weak learner note although difference interestingly weighting closely resembles gaussian weighting employed sift low dimensional boosted gradient map improve performance optimize correlation matrix weak learner response explained sec apply embedding sec result method shown fig b experiment learn l bgm descriptor using response gradient based weak learner selected boosting first optimize weak learner correlation matrix constrained diagonal corresponds global optimization weight weak learner resulting dimensional l bgm diag descriptor performs slightly better corresponding dimensional bgm interestingly additional degree freedom obtained optimizing full correlation matrix boost result significantly allow u outperform sift dimension compare dimensional descriptor ie descriptor length sift observe improvement term error rate however increase descriptor length see slight performance drop since begin include noisy dimension embedding correspond eigenvalue low magnitude trend typical many dimensionality reduction technique hence final descriptor select dimensional l bgm descriptor provides decent trade performance descriptor length figure b also show result obtained applying pca response gradient based weak learner bgm pca descriptor generated way performs similarly sift however method still provides better result even dimensionality show advantage optimizing exponential loss eq comparison state art compare approach following baseline sum squared difference pixel intensity ssd state art sift descriptor surf descriptor binary ldahash descriptor real valued descriptor computed applying lde projection bias gain normalized patch lda int original boosted ssc also tested recent binary descriptor brief orb brisk however performed much worse baseline presented paper sift use publicly available implementation vedaldi surf ldahash use implementation available website author method use implementation lda int choose dimensionality reported perform best given dataset according boosted ssc use dimension obtained best performance train yosemite k test notre dame k true positive rate true positive rate train notre dame k test liberty k ssd sift surf ldahash lda int boosted ssc bgm l bgm ssd sift surf ldahash lda int boosted ssc bgm l bgm false positive rate false positive rate b figure comparison state art parenthesis number dimension error rate l bgm approach outperforms sift term error rate using half fewer dimension fig plot recognition curve baseline method bgm l bgm outperform baseline method across fp rate maximal performance boost obtained using dimensional l bgm descriptor result improvement term error rate respect state art sift descriptor descriptor derived patch intensity ie ssd boosted ssc lda int perform much worse gradient based one finally bgm l bgm descriptor far outperform sift relies hand crafted filter applied gradient map moreover bgm l bgm able reduce error rate time respect state art descriptor namely surf ldahash computed result configuration training testing datasets without observing significant difference thus show representative set curve result found supplementary material interestingly result obtain comparable best best result reported however since code compact descriptor publicly available compare performance term error rate composite descriptor provide advantage compact l bgm average error rate lower l bgm nevertheless outperform non parametric descriptor perform slightly better parametric one using descriptor order magnitude shorter comparison indicates even though approach require complex pipeline optimization parameter tuning perform similarly finely optimized descriptor presented conclusion paper presented new method learning image descriptor using low dimensional boosted gradient map l bgm l bgm offer attractive alternative traditional descriptor learning technique model non linearity based kernel trick relying pre specified kernel function whose selection difficult unintuitive contrast shown descriptor matching problem boosting trick lead non linear feature mapping whose feature intuitive explanation demonstrated use gradient based weak learner function learning descriptor within framework illustrating close connection well known sift descriptor discriminative embedding technique also presented yielding fairly compact discriminative feature description compared baseline method evaluated approach benchmark datasets l bgm shown outperform leading contemporary hand designed learned feature descriptor unlike previous approach l bgm descriptor learned directly raw intensity patch achieving state art performance interesting avenue future work include exploration weak learner family descriptor learning eg surf like haar feature extension binary feature embeddings acknowledgment would like thank karim ali sharing feature code insightful feedback discussion reference lowe distinctive image feature scale invariant keypoints ijcv bay h tuytelaars van gool l surf speeded robust feature eccv shakhnarovich g learning task specific similarity phd thesis mit brown hua g winder discriminative learning local image descriptor pami strecha c bronstein bronstein fua p ldahash improved matching smaller descriptor pami kulis b jain p grauman k fast similarity search learned metric pami shen c kim j wang l van den hengel positive semidefinite metric learning boosting nip jain p kulis b davis j dhillon metric kernel learning using linear transformation jmlr bi j wu lu l liu tao wolf adaboost low rank psd matrix metric learning cvpr viola p jones rapid object detection using boosted cascade simple feature cvpr chapelle shivaswamy p vadrevu weinberger k zhang tseng b boosted multi task learning machine learning ali k fleuret f hasler fua p real time deformable detector pami dalal n triggs b histogram oriented gradient human detection cvpr wei torralba fergus r spectral hashing nip kulis b darrell learning hash binary reconstructive embeddings nip salakhutdinov r hinton g learning nonlinear embedding preserving class neighbourhood structure international conference artificial intelligence statistic salakhutdinov r hinton g semantic hashing international journal approximate reasoning grauman k darrell pyramid match kernel discriminative classification set image feature iccv shen c welsh wang l psdboost matrix generation linear programming positive semidefinite matrix learning nip jia huang c darrell beyond spatial pyramid receptive field learning pooled image feature cvpr simonyan k vedaldi zisserman descriptor learning using convex optimisation eccv dollar p tu z perona p belongie integral channel feature bmvc torralba fergus r wei small code large database recognition cvpr ali k fleuret f hasler fua p real time deformable detector pami freund schapire r decision theoretic generalization line learning application boosting european conference computational learning theory rosset zhu j hastie boosting regularized path maximum margin classifier jmlr calonder lepetit v ozuysal trzcinski strecha c fua p brief computing local binary descriptor fast pami rublee e rabaud v konolidge k bradski g orb efficient alternative sift surf iccv leutenegger chli siegwart r brisk binary robust invariant scalable keypoints iccv vedaldi http wwwvlfeatorg vedaldi code siftpphtml'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# #text preprocessing\n",
        "# import nltk\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "# # sen = nltk.sent_tokenize(paragraph)\n",
        "# # words = nltk.word_tokenize(paragraph)\n",
        "# nltk.download('wordnet')\n",
        "# #lowering case, remove stop words,punctuations and apply stemming or lemmatizing (technqiues to find root word)\n",
        "# #stemming - stems or cuts last few char. and might not represtn a meaningful word\n",
        "# #lemmatize - it takes context of word and convets to a meaningful word.helpful in chatbots(time taking process)\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# #from nltk.stem import PorterStemmer\n",
        "# import re\n",
        "# corpus = []\n",
        "# ps= WordNetLemmatizer()\n",
        "# removelist=\"\"\n",
        "# for i in range(len(papers['paper_text'])):\n",
        "\n",
        "#   words =re.sub('[,\\.!?]','',papers['paper_text'][i])          #remove punctuations\n",
        "#   #review2= re.sub('https://.*','',review1)   #remove URLs\n",
        "#   words = re.sub('[^a-zA-Z]', ' ',words)\n",
        "#   words = words.lower()\n",
        "#   words = words.split()\n",
        "#   words= [ps.lemmatize(word) for word in words if word not in stopwords.words('english')]\n",
        "#   words = ' '.join(words)\n",
        "#   corpus.append(words)\n",
        "\n",
        "# corpus[4]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBULNEaY3oVR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDK5wEL2ynaS",
        "outputId": "43b170b7-60d2-40e4-8546-28bd9dfec6d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# lst =[]\n",
        "# nltk.download('punkt')\n",
        "# for i in range(len(corpus)):\n",
        "#   lst.append(corpus[i].split())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XmSViENY0O8i"
      },
      "outputs": [],
      "source": [
        "import gensim.corpora as corpora\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict(id2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tevvrIXYoXHL",
        "outputId": "d43ccc04-e82a-462c-dbff-7fe073929c2c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'aa',\n",
              " 1: 'aat',\n",
              " 2: 'ability',\n",
              " 3: 'able',\n",
              " 4: 'abs',\n",
              " 5: 'abstract',\n",
              " 6: 'academic',\n",
              " 7: 'according',\n",
              " 8: 'accurate',\n",
              " 9: 'achieve',\n",
              " 10: 'acknowledge',\n",
              " 11: 'acknowledgments',\n",
              " 12: 'acta',\n",
              " 13: 'active',\n",
              " 14: 'actively',\n",
              " 15: 'adaptive',\n",
              " 16: 'add',\n",
              " 17: 'additional',\n",
              " 18: 'additive',\n",
              " 19: 'affine',\n",
              " 20: 'affords',\n",
              " 21: 'agrees',\n",
              " 22: 'ai',\n",
              " 23: 'aij',\n",
              " 24: 'ak',\n",
              " 25: 'algorithm',\n",
              " 26: 'algorithmic',\n",
              " 27: 'algorithms',\n",
              " 28: 'allowed',\n",
              " 29: 'allows',\n",
              " 30: 'almost',\n",
              " 31: 'along',\n",
              " 32: 'also',\n",
              " 33: 'alternatives',\n",
              " 34: 'amer',\n",
              " 35: 'american',\n",
              " 36: 'among',\n",
              " 37: 'amounts',\n",
              " 38: 'amplified',\n",
              " 39: 'amplifying',\n",
              " 40: 'amx',\n",
              " 41: 'anal',\n",
              " 42: 'analysis',\n",
              " 43: 'analytically',\n",
              " 44: 'analyze',\n",
              " 45: 'anastasios',\n",
              " 46: 'ann',\n",
              " 47: 'annals',\n",
              " 48: 'appear',\n",
              " 49: 'appearance',\n",
              " 50: 'appears',\n",
              " 51: 'appl',\n",
              " 52: 'applications',\n",
              " 53: 'applied',\n",
              " 54: 'apply',\n",
              " 55: 'approach',\n",
              " 56: 'approaches',\n",
              " 57: 'approx',\n",
              " 58: 'approximate',\n",
              " 59: 'approximately',\n",
              " 60: 'approximation',\n",
              " 61: 'arbitrary',\n",
              " 62: 'arg',\n",
              " 63: 'argument',\n",
              " 64: 'aro',\n",
              " 65: 'around',\n",
              " 66: 'assoc',\n",
              " 67: 'associated',\n",
              " 68: 'association',\n",
              " 69: 'assume',\n",
              " 70: 'assuming',\n",
              " 71: 'assumption',\n",
              " 72: 'assumptions',\n",
              " 73: 'asymptotic',\n",
              " 74: 'ati',\n",
              " 75: 'atoms',\n",
              " 76: 'attempt',\n",
              " 77: 'august',\n",
              " 78: 'authors',\n",
              " 79: 'average',\n",
              " 80: 'averaged',\n",
              " 81: 'averaging',\n",
              " 82: 'ax',\n",
              " 83: 'back',\n",
              " 84: 'background',\n",
              " 85: 'balance',\n",
              " 86: 'ball',\n",
              " 87: 'bandit',\n",
              " 88: 'baraniuk',\n",
              " 89: 'based',\n",
              " 90: 'basis',\n",
              " 91: 'bd',\n",
              " 92: 'becomes',\n",
              " 93: 'berkeley',\n",
              " 94: 'bernoulli',\n",
              " 95: 'best',\n",
              " 96: 'better',\n",
              " 97: 'beyond',\n",
              " 98: 'bi',\n",
              " 99: 'bk',\n",
              " 100: 'body',\n",
              " 101: 'bound',\n",
              " 102: 'bounded',\n",
              " 103: 'bounds',\n",
              " 104: 'brd',\n",
              " 105: 'bridge',\n",
              " 106: 'brk',\n",
              " 107: 'bt',\n",
              " 108: 'buhlmann',\n",
              " 109: 'bv',\n",
              " 110: 'calculate',\n",
              " 111: 'cambridge',\n",
              " 112: 'cand',\n",
              " 113: 'cannot',\n",
              " 114: 'capturing',\n",
              " 115: 'carin',\n",
              " 116: 'carry',\n",
              " 117: 'case',\n",
              " 118: 'cases',\n",
              " 119: 'categorize',\n",
              " 120: 'center',\n",
              " 121: 'centers',\n",
              " 122: 'cevher',\n",
              " 123: 'cf',\n",
              " 124: 'challenging',\n",
              " 125: 'changes',\n",
              " 126: 'changing',\n",
              " 127: 'chapter',\n",
              " 128: 'characterize',\n",
              " 129: 'characterizes',\n",
              " 130: 'choose',\n",
              " 131: 'choosing',\n",
              " 132: 'chosen',\n",
              " 133: 'circles',\n",
              " 134: 'classes',\n",
              " 135: 'classical',\n",
              " 136: 'cohen',\n",
              " 137: 'collateral',\n",
              " 138: 'commission',\n",
              " 139: 'compact',\n",
              " 140: 'completion',\n",
              " 141: 'complex',\n",
              " 142: 'complexity',\n",
              " 143: 'component',\n",
              " 144: 'compressibility',\n",
              " 145: 'compressible',\n",
              " 146: 'comput',\n",
              " 147: 'computation',\n",
              " 148: 'computational',\n",
              " 149: 'computationally',\n",
              " 150: 'compute',\n",
              " 151: 'concentration',\n",
              " 152: 'concern',\n",
              " 153: 'concluding',\n",
              " 154: 'conclusions',\n",
              " 155: 'condition',\n",
              " 156: 'conditioned',\n",
              " 157: 'confused',\n",
              " 158: 'consequence',\n",
              " 159: 'consider',\n",
              " 160: 'constant',\n",
              " 161: 'constants',\n",
              " 162: 'constr',\n",
              " 163: 'constraining',\n",
              " 164: 'construct',\n",
              " 165: 'construction',\n",
              " 166: 'constructive',\n",
              " 167: 'constructs',\n",
              " 168: 'context',\n",
              " 169: 'continuous',\n",
              " 170: 'continuously',\n",
              " 171: 'contrast',\n",
              " 172: 'contribution',\n",
              " 173: 'contributions',\n",
              " 174: 'control',\n",
              " 175: 'conventional',\n",
              " 176: 'convex',\n",
              " 177: 'coordinate',\n",
              " 178: 'coordinates',\n",
              " 179: 'corollary',\n",
              " 180: 'corr',\n",
              " 181: 'corresponding',\n",
              " 182: 'corrupted',\n",
              " 183: 'cost',\n",
              " 184: 'course',\n",
              " 185: 'covering',\n",
              " 186: 'criteria',\n",
              " 187: 'critical',\n",
              " 188: 'curse',\n",
              " 189: 'curvature',\n",
              " 190: 'damage',\n",
              " 191: 'dantzig',\n",
              " 192: 'danzig',\n",
              " 193: 'darpa',\n",
              " 194: 'dashed',\n",
              " 195: 'data',\n",
              " 196: 'daubechies',\n",
              " 197: 'de',\n",
              " 198: 'declare',\n",
              " 199: 'decompose',\n",
              " 200: 'decomposition',\n",
              " 201: 'deficient',\n",
              " 202: 'define',\n",
              " 203: 'defined',\n",
              " 204: 'definition',\n",
              " 205: 'demonstrate',\n",
              " 206: 'denote',\n",
              " 207: 'denoting',\n",
              " 208: 'department',\n",
              " 209: 'dependence',\n",
              " 210: 'depends',\n",
              " 211: 'depicts',\n",
              " 212: 'derivative',\n",
              " 213: 'derivatives',\n",
              " 214: 'derive',\n",
              " 215: 'derived',\n",
              " 216: 'described',\n",
              " 217: 'designed',\n",
              " 218: 'details',\n",
              " 219: 'developments',\n",
              " 220: 'deviation',\n",
              " 221: 'deviations',\n",
              " 222: 'devore',\n",
              " 223: 'different',\n",
              " 224: 'differentiable',\n",
              " 225: 'differential',\n",
              " 226: 'differently',\n",
              " 227: 'difficult',\n",
              " 228: 'dimension',\n",
              " 229: 'dimensional',\n",
              " 230: 'dimensionality',\n",
              " 231: 'dimensions',\n",
              " 232: 'direction',\n",
              " 233: 'directional',\n",
              " 234: 'directions',\n",
              " 235: 'discuss',\n",
              " 236: 'discussion',\n",
              " 237: 'discussions',\n",
              " 238: 'distributed',\n",
              " 239: 'distribution',\n",
              " 240: 'dl',\n",
              " 241: 'dm',\n",
              " 242: 'dmx',\n",
              " 243: 'domain',\n",
              " 244: 'donoho',\n",
              " 245: 'ds',\n",
              " 246: 'dsd',\n",
              " 247: 'duality',\n",
              " 248: 'due',\n",
              " 249: 'dunson',\n",
              " 250: 'earlier',\n",
              " 251: 'easily',\n",
              " 252: 'easy',\n",
              " 253: 'econometrics',\n",
              " 254: 'effectively',\n",
              " 255: 'effectiveness',\n",
              " 256: 'effects',\n",
              " 257: 'eg',\n",
              " 258: 'ej',\n",
              " 259: 'elementary',\n",
              " 260: 'elements',\n",
              " 261: 'em',\n",
              " 262: 'embedding',\n",
              " 263: 'empirical',\n",
              " 264: 'empirically',\n",
              " 265: 'employ',\n",
              " 266: 'employed',\n",
              " 267: 'enable',\n",
              " 268: 'enables',\n",
              " 269: 'encode',\n",
              " 270: 'encoded',\n",
              " 271: 'end',\n",
              " 272: 'engineering',\n",
              " 273: 'enlargement',\n",
              " 274: 'enovak',\n",
              " 275: 'ensemble',\n",
              " 276: 'ensuing',\n",
              " 277: 'ensures',\n",
              " 278: 'ensuring',\n",
              " 279: 'epfl',\n",
              " 280: 'equalities',\n",
              " 281: 'equation',\n",
              " 282: 'equations',\n",
              " 283: 'erc',\n",
              " 284: 'error',\n",
              " 285: 'errors',\n",
              " 286: 'es',\n",
              " 287: 'establish',\n",
              " 288: 'establishing',\n",
              " 289: 'estimate',\n",
              " 290: 'estimated',\n",
              " 291: 'estimates',\n",
              " 292: 'estimating',\n",
              " 293: 'estimation',\n",
              " 294: 'estimator',\n",
              " 295: 'european',\n",
              " 296: 'evaluation',\n",
              " 297: 'evaluations',\n",
              " 298: 'even',\n",
              " 299: 'everywhere',\n",
              " 300: 'evidence',\n",
              " 301: 'exact',\n",
              " 302: 'example',\n",
              " 303: 'examples',\n",
              " 304: 'exist',\n",
              " 305: 'exists',\n",
              " 306: 'exp',\n",
              " 307: 'expansion',\n",
              " 308: 'expensive',\n",
              " 309: 'experiments',\n",
              " 310: 'explain',\n",
              " 311: 'explicit',\n",
              " 312: 'explicitly',\n",
              " 313: 'exploit',\n",
              " 314: 'explores',\n",
              " 315: 'exponential',\n",
              " 316: 'expose',\n",
              " 317: 'extended',\n",
              " 318: 'extensive',\n",
              " 319: 'ey',\n",
              " 320: 'factor',\n",
              " 321: 'faculty',\n",
              " 322: 'far',\n",
              " 323: 'fashion',\n",
              " 324: 'fazel',\n",
              " 325: 'fb',\n",
              " 326: 'fbkl',\n",
              " 327: 'fea',\n",
              " 328: 'feasible',\n",
              " 329: 'fellowship',\n",
              " 330: 'figure',\n",
              " 331: 'final',\n",
              " 332: 'finally',\n",
              " 333: 'finding',\n",
              " 334: 'finite',\n",
              " 335: 'first',\n",
              " 336: 'fitting',\n",
              " 337: 'fix',\n",
              " 338: 'fixed',\n",
              " 339: 'focus',\n",
              " 340: 'fold',\n",
              " 341: 'followed',\n",
              " 342: 'following',\n",
              " 343: 'follows',\n",
              " 344: 'form',\n",
              " 345: 'forms',\n",
              " 346: 'formulation',\n",
              " 347: 'fornasier',\n",
              " 348: 'fortunately',\n",
              " 349: 'found',\n",
              " 350: 'foundations',\n",
              " 351: 'free',\n",
              " 352: 'friedman',\n",
              " 353: 'full',\n",
              " 354: 'fully',\n",
              " 355: 'function',\n",
              " 356: 'functional',\n",
              " 357: 'functions',\n",
              " 358: 'fundamental',\n",
              " 359: 'furthermore',\n",
              " 360: 'future',\n",
              " 361: 'gaussian',\n",
              " 362: 'gb',\n",
              " 363: 'geer',\n",
              " 364: 'general',\n",
              " 365: 'generality',\n",
              " 366: 'generalization',\n",
              " 367: 'generalizes',\n",
              " 368: 'generate',\n",
              " 369: 'gets',\n",
              " 370: 'gg',\n",
              " 371: 'gh',\n",
              " 372: 'gi',\n",
              " 373: 'given',\n",
              " 374: 'governs',\n",
              " 375: 'gradients',\n",
              " 376: 'grant',\n",
              " 377: 'greater',\n",
              " 378: 'greedy',\n",
              " 379: 'grid',\n",
              " 380: 'grow',\n",
              " 381: 'grows',\n",
              " 382: 'grundlehren',\n",
              " 383: 'guarantee',\n",
              " 384: 'guaranteed',\n",
              " 385: 'guarantees',\n",
              " 386: 'gw',\n",
              " 387: 'haar',\n",
              " 388: 'hall',\n",
              " 389: 'handle',\n",
              " 390: 'hardle',\n",
              " 391: 'harmon',\n",
              " 392: 'harmonic',\n",
              " 393: 'help',\n",
              " 394: 'hemant',\n",
              " 395: 'hence',\n",
              " 396: 'hessian',\n",
              " 397: 'hh',\n",
              " 398: 'high',\n",
              " 399: 'highdimensional',\n",
              " 400: 'highlight',\n",
              " 401: 'hilbert',\n",
              " 402: 'holds',\n",
              " 403: 'however',\n",
              " 404: 'hristache',\n",
              " 405: 'huber',\n",
              " 406: 'hwoniakowski',\n",
              " 407: 'hzk',\n",
              " 408: 'icassp',\n",
              " 409: 'identically',\n",
              " 410: 'identify',\n",
              " 411: 'ie',\n",
              " 412: 'ieee',\n",
              " 413: 'iid',\n",
              " 414: 'ij',\n",
              " 415: 'ijk',\n",
              " 416: 'ik',\n",
              " 417: 'im',\n",
              " 418: 'immune',\n",
              " 419: 'impact',\n",
              " 420: 'impacts',\n",
              " 421: 'implicit',\n",
              " 422: 'importance',\n",
              " 423: 'important',\n",
              " 424: 'improve',\n",
              " 425: 'inc',\n",
              " 426: 'incomplete',\n",
              " 427: 'increase',\n",
              " 428: 'incurs',\n",
              " 429: 'independent',\n",
              " 430: 'index',\n",
              " 431: 'individual',\n",
              " 432: 'individually',\n",
              " 433: 'inf',\n",
              " 434: 'infinitely',\n",
              " 435: 'information',\n",
              " 436: 'infoscience',\n",
              " 437: 'instance',\n",
              " 438: 'intended',\n",
              " 439: 'interaction',\n",
              " 440: 'interested',\n",
              " 441: 'interpolants',\n",
              " 442: 'interpolate',\n",
              " 443: 'intractable',\n",
              " 444: 'introduce',\n",
              " 445: 'introducing',\n",
              " 446: 'introduction',\n",
              " 447: 'inverse',\n",
              " 448: 'inversely',\n",
              " 449: 'involved',\n",
              " 450: 'isometry',\n",
              " 451: 'ity',\n",
              " 452: 'jan',\n",
              " 453: 'jf',\n",
              " 454: 'jh',\n",
              " 455: 'johnstone',\n",
              " 456: 'jordan',\n",
              " 457: 'journal',\n",
              " 458: 'juditsky',\n",
              " 459: 'kakade',\n",
              " 460: 'kax',\n",
              " 461: 'kaxk',\n",
              " 462: 'kc',\n",
              " 463: 'kdimensional',\n",
              " 464: 'kecom',\n",
              " 465: 'keep',\n",
              " 466: 'kerkyacharian',\n",
              " 467: 'kernel',\n",
              " 468: 'kernels',\n",
              " 469: 'key',\n",
              " 470: 'kf',\n",
              " 471: 'kk',\n",
              " 472: 'km',\n",
              " 473: 'kmx',\n",
              " 474: 'known',\n",
              " 475: 'koltchinskii',\n",
              " 476: 'krause',\n",
              " 477: 'kx',\n",
              " 478: 'kxk',\n",
              " 479: 'kxkf',\n",
              " 480: 'ky',\n",
              " 481: 'kyrillidis',\n",
              " 482: 'kzk',\n",
              " 483: 'lafferty',\n",
              " 484: 'large',\n",
              " 485: 'largest',\n",
              " 486: 'lastly',\n",
              " 487: 'laurent',\n",
              " 488: 'lead',\n",
              " 489: 'leads',\n",
              " 490: 'learn',\n",
              " 491: 'learned',\n",
              " 492: 'learner',\n",
              " 493: 'learning',\n",
              " 494: 'least',\n",
              " 495: 'left',\n",
              " 496: 'lemma',\n",
              " 497: 'less',\n",
              " 498: 'lest',\n",
              " 499: 'let',\n",
              " 500: 'leverage',\n",
              " 501: 'li',\n",
              " 502: 'lifts',\n",
              " 503: 'lij',\n",
              " 504: 'like',\n",
              " 505: 'limited',\n",
              " 506: 'lin',\n",
              " 507: 'linear',\n",
              " 508: 'linearity',\n",
              " 509: 'linearly',\n",
              " 510: 'lines',\n",
              " 511: 'lions',\n",
              " 512: 'lipschitz',\n",
              " 513: 'literature',\n",
              " 514: 'liu',\n",
              " 515: 'live',\n",
              " 516: 'lives',\n",
              " 517: 'local',\n",
              " 518: 'location',\n",
              " 519: 'log',\n",
              " 520: 'logarithmically',\n",
              " 521: 'logistic',\n",
              " 522: 'lorentz',\n",
              " 523: 'loss',\n",
              " 524: 'low',\n",
              " 525: 'lower',\n",
              " 526: 'lowerbound',\n",
              " 527: 'lx',\n",
              " 528: 'machine',\n",
              " 529: 'made',\n",
              " 530: 'magazine',\n",
              " 531: 'main',\n",
              " 532: 'majority',\n",
              " 533: 'make',\n",
              " 534: 'makes',\n",
              " 535: 'manages',\n",
              " 536: 'many',\n",
              " 537: 'mapping',\n",
              " 538: 'maps',\n",
              " 539: 'massart',\n",
              " 540: 'match',\n",
              " 541: 'mathematics',\n",
              " 542: 'matrices',\n",
              " 543: 'matrix',\n",
              " 544: 'max',\n",
              " 545: 'maximum',\n",
              " 546: 'may',\n",
              " 547: 'mb',\n",
              " 548: 'mean',\n",
              " 549: 'measure',\n",
              " 550: 'measurement',\n",
              " 551: 'measurements',\n",
              " 552: 'mechanism',\n",
              " 553: 'meier',\n",
              " 554: 'mention',\n",
              " 555: 'methodology',\n",
              " 556: 'methods',\n",
              " 557: 'mi',\n",
              " 558: 'might',\n",
              " 559: 'min',\n",
              " 560: 'mind',\n",
              " 561: 'minimal',\n",
              " 562: 'minimax',\n",
              " 563: 'minimization',\n",
              " 564: 'minimize',\n",
              " 565: 'minimum',\n",
              " 566: 'mirg',\n",
              " 567: 'mlp',\n",
              " 568: 'model',\n",
              " 569: 'modeling',\n",
              " 570: 'models',\n",
              " 571: 'modular',\n",
              " 572: 'moreover',\n",
              " 573: 'motivates',\n",
              " 574: 'motivation',\n",
              " 575: 'multi',\n",
              " 576: 'multiple',\n",
              " 577: 'multivariate',\n",
              " 578: 'muri',\n",
              " 579: 'mx',\n",
              " 580: 'near',\n",
              " 581: 'nearest',\n",
              " 582: 'necessarily',\n",
              " 583: 'necessary',\n",
              " 584: 'need',\n",
              " 585: 'needed',\n",
              " 586: 'neighbor',\n",
              " 587: 'neighborhood',\n",
              " 588: 'networks',\n",
              " 589: 'neural',\n",
              " 590: 'new',\n",
              " 591: 'next',\n",
              " 592: 'nf',\n",
              " 593: 'noise',\n",
              " 594: 'noisy',\n",
              " 595: 'non',\n",
              " 596: 'nonparametric',\n",
              " 597: 'norm',\n",
              " 598: 'norms',\n",
              " 599: 'note',\n",
              " 600: 'noting',\n",
              " 601: 'nuclear',\n",
              " 602: 'number',\n",
              " 603: 'numerica',\n",
              " 604: 'numerical',\n",
              " 605: 'ny',\n",
              " 606: 'obeys',\n",
              " 607: 'objectives',\n",
              " 608: 'observation',\n",
              " 609: 'observe',\n",
              " 610: 'obtain',\n",
              " 611: 'obtained',\n",
              " 612: 'obtaining',\n",
              " 613: 'offer',\n",
              " 614: 'offs',\n",
              " 615: 'often',\n",
              " 616: 'one',\n",
              " 617: 'online',\n",
              " 618: 'open',\n",
              " 619: 'operator',\n",
              " 620: 'operators',\n",
              " 621: 'opposed',\n",
              " 622: 'optimal',\n",
              " 623: 'optimization',\n",
              " 624: 'optimize',\n",
              " 625: 'oracle',\n",
              " 626: 'order',\n",
              " 627: 'origin',\n",
              " 628: 'orthogonal',\n",
              " 629: 'orthonormal',\n",
              " 630: 'outlines',\n",
              " 631: 'overview',\n",
              " 632: 'overwhelming',\n",
              " 633: 'pa',\n",
              " 634: 'pages',\n",
              " 635: 'paper',\n",
              " 636: 'parameter',\n",
              " 637: 'parameters',\n",
              " 638: 'parametric',\n",
              " 639: 'parrilo',\n",
              " 640: 'part',\n",
              " 641: 'partial',\n",
              " 642: 'particular',\n",
              " 643: 'particularly',\n",
              " 644: 'passive',\n",
              " 645: 'pd',\n",
              " 646: 'pde',\n",
              " 647: 'per',\n",
              " 648: 'performance',\n",
              " 649: 'perspective',\n",
              " 650: 'perturb',\n",
              " 651: 'perturbations',\n",
              " 652: 'perturbed',\n",
              " 653: 'picard',\n",
              " 654: 'pick',\n",
              " 655: 'pinkus',\n",
              " 656: 'pj',\n",
              " 657: 'pk',\n",
              " 658: 'plan',\n",
              " 659: 'played',\n",
              " 660: 'plot',\n",
              " 661: 'pmx',\n",
              " 662: 'point',\n",
              " 663: 'points',\n",
              " 664: 'polzehl',\n",
              " 665: 'posit',\n",
              " 666: 'possible',\n",
              " 667: 'power',\n",
              " 668: 'precise',\n",
              " 669: 'preliminaries',\n",
              " 670: 'preprint',\n",
              " 671: 'prescribed',\n",
              " 672: 'present',\n",
              " 673: 'preserved',\n",
              " 674: 'press',\n",
              " 675: 'principal',\n",
              " 676: 'priori',\n",
              " 677: 'probabil',\n",
              " 678: 'probability',\n",
              " 679: 'problem',\n",
              " 680: 'process',\n",
              " 681: 'processing',\n",
              " 682: 'program',\n",
              " 683: 'programming',\n",
              " 684: 'projection',\n",
              " 685: 'projections',\n",
              " 686: 'proof',\n",
              " 687: 'property',\n",
              " 688: 'proportional',\n",
              " 689: 'propose',\n",
              " 690: 'proposition',\n",
              " 691: 'prove',\n",
              " 692: 'proved',\n",
              " 693: 'provide',\n",
              " 694: 'provides',\n",
              " 695: 'proving',\n",
              " 696: 'pursuit',\n",
              " 697: 'quadratic',\n",
              " 698: 'qualitatively',\n",
              " 699: 'quasi',\n",
              " 700: 'queries',\n",
              " 701: 'query',\n",
              " 702: 'quite',\n",
              " 703: 'radial',\n",
              " 704: 'radius',\n",
              " 705: 'random',\n",
              " 706: 'randomized',\n",
              " 707: 'range',\n",
              " 708: 'rank',\n",
              " 709: 'raskutti',\n",
              " 710: 'rates',\n",
              " 711: 'rather',\n",
              " 712: 'ratio',\n",
              " 713: 'ravikumar',\n",
              " 714: 'rd',\n",
              " 715: 'rdmx',\n",
              " 716: 'reaches',\n",
              " 717: 'reader',\n",
              " 718: 'realistic',\n",
              " 719: 'recall',\n",
              " 720: 'recent',\n",
              " 721: 'recently',\n",
              " 722: 'recht',\n",
              " 723: 'recipe',\n",
              " 724: 'recovered',\n",
              " 725: 'recovery',\n",
              " 726: 'rectangular',\n",
              " 727: 'reduced',\n",
              " 728: 'reduces',\n",
              " 729: 'reduction',\n",
              " 730: 'redundant',\n",
              " 731: 'refer',\n",
              " 732: 'references',\n",
              " 733: 'regarding',\n",
              " 734: 'regression',\n",
              " 735: 'regret',\n",
              " 736: 'regularizers',\n",
              " 737: 'relative',\n",
              " 738: 'relaxation',\n",
              " 739: 'relies',\n",
              " 740: 'remain',\n",
              " 741: 'remark',\n",
              " 742: 'remedy',\n",
              " 743: 'remove',\n",
              " 744: 'repeatedly',\n",
              " 745: 'report',\n",
              " 746: 'reproducing',\n",
              " 747: 'require',\n",
              " 748: 'requires',\n",
              " 749: 'resample',\n",
              " 750: 'respectively',\n",
              " 751: 'restate',\n",
              " 752: 'restrict',\n",
              " 753: 'restricted',\n",
              " 754: 'restriction',\n",
              " 755: 'restrictions',\n",
              " 756: 'restrictive',\n",
              " 757: 'result',\n",
              " 758: 'results',\n",
              " 759: 'returns',\n",
              " 760: 'review',\n",
              " 761: 'revolve',\n",
              " 762: 'revolves',\n",
              " 763: 'rg',\n",
              " 764: 'rice',\n",
              " 765: 'richness',\n",
              " 766: 'ridge',\n",
              " 767: 'rigorous',\n",
              " 768: 'rigorously',\n",
              " 769: 'rip',\n",
              " 770: 'rkd',\n",
              " 771: 'rm',\n",
              " 772: 'robustness',\n",
              " 773: 'role',\n",
              " 774: 'rotation',\n",
              " 775: 'row',\n",
              " 776: 'rows',\n",
              " 777: 'royal',\n",
              " 778: 'running',\n",
              " 779: 'sample',\n",
              " 780: 'sampled',\n",
              " 781: 'samples',\n",
              " 782: 'sampling',\n",
              " 783: 'sapiro',\n",
              " 784: 'satisfies',\n",
              " 785: 'satisfy',\n",
              " 786: 'say',\n",
              " 787: 'scale',\n",
              " 788: 'scaled',\n",
              " 789: 'scales',\n",
              " 790: 'scaling',\n",
              " 791: 'scheme',\n",
              " 792: 'schemes',\n",
              " 793: 'schnass',\n",
              " 794: 'scientific',\n",
              " 795: 'sd',\n",
              " 796: 'second',\n",
              " 797: 'section',\n",
              " 798: 'see',\n",
              " 799: 'seeger',\n",
              " 800: 'seek',\n",
              " 801: 'select',\n",
              " 802: 'selecting',\n",
              " 803: 'selection',\n",
              " 804: 'selector',\n",
              " 805: 'sense',\n",
              " 806: 'sensitive',\n",
              " 807: 'sequel',\n",
              " 808: 'series',\n",
              " 809: 'set',\n",
              " 810: 'sets',\n",
              " 811: 'setting',\n",
              " 812: 'several',\n",
              " 813: 'show',\n",
              " 814: 'shown',\n",
              " 815: 'siam',\n",
              " 816: 'signal',\n",
              " 817: 'simplicity',\n",
              " 818: 'simply',\n",
              " 819: 'simulation',\n",
              " 820: 'simulations',\n",
              " 821: 'since',\n",
              " 822: 'singular',\n",
              " 823: 'size',\n",
              " 824: 'sk',\n",
              " 825: 'sliced',\n",
              " 826: 'small',\n",
              " 827: 'smaller',\n",
              " 828: 'smooth',\n",
              " 829: 'smoothing',\n",
              " 830: 'smoothness',\n",
              " 831: 'snf',\n",
              " 832: 'sobolev',\n",
              " 833: 'society',\n",
              " 834: 'solution',\n",
              " 835: 'solutions',\n",
              " 836: 'solver',\n",
              " 837: 'space',\n",
              " 838: 'spaced',\n",
              " 839: 'spaces',\n",
              " 840: 'span',\n",
              " 841: 'spanned',\n",
              " 842: 'sparse',\n",
              " 843: 'sparsify',\n",
              " 844: 'sparsity',\n",
              " 845: 'specific',\n",
              " 846: 'specified',\n",
              " 847: 'spectral',\n",
              " 848: 'sphere',\n",
              " 849: 'spline',\n",
              " 850: 'spokoiny',\n",
              " 851: 'springer',\n",
              " 852: 'srinivas',\n",
              " 853: 'st',\n",
              " 854: 'stability',\n",
              " 855: 'stable',\n",
              " 856: 'stage',\n",
              " 857: 'standard',\n",
              " 858: 'stark',\n",
              " 859: 'stating',\n",
              " 860: 'statist',\n",
              " 861: 'statistical',\n",
              " 862: 'statistics',\n",
              " 863: 'step',\n",
              " 864: 'steps',\n",
              " 865: 'stone',\n",
              " 866: 'straightforward',\n",
              " 867: 'structure',\n",
              " 868: 'study',\n",
              " 869: 'stuetzel',\n",
              " 870: 'stylized',\n",
              " 871: 'subsequently',\n",
              " 872: 'subspace',\n",
              " 873: 'subspaces',\n",
              " 874: 'substituting',\n",
              " 875: 'successful',\n",
              " 876: 'successfully',\n",
              " 877: 'sufficient',\n",
              " 878: 'sufficiently',\n",
              " 879: 'sum',\n",
              " 880: 'summarize',\n",
              " 881: 'summary',\n",
              " 882: 'sup',\n",
              " 883: 'supported',\n",
              " 884: 'supports',\n",
              " 885: 'surprisingly',\n",
              " 886: 'svd',\n",
              " 887: 'system',\n",
              " 888: 'take',\n",
              " 889: 'tao',\n",
              " 890: 'taylor',\n",
              " 891: 'technical',\n",
              " 892: 'techniques',\n",
              " 893: 'term',\n",
              " 894: 'th',\n",
              " 895: 'thank',\n",
              " 896: 'theor',\n",
              " 897: 'theorem',\n",
              " 898: 'theoretic',\n",
              " 899: 'theoretical',\n",
              " 900: 'theory',\n",
              " 901: 'thereby',\n",
              " 902: 'therefore',\n",
              " 903: 'third',\n",
              " 904: 'three',\n",
              " 905: 'thus',\n",
              " 906: 'tight',\n",
              " 907: 'tightened',\n",
              " 908: 'tightness',\n",
              " 909: 'till',\n",
              " 910: 'times',\n",
              " 911: 'tj',\n",
              " 912: 'together',\n",
              " 913: 'tong',\n",
              " 914: 'tool',\n",
              " 915: 'toy',\n",
              " 916: 'trace',\n",
              " 917: 'tractability',\n",
              " 918: 'trade',\n",
              " 919: 'training',\n",
              " 920: 'trans',\n",
              " 921: 'transformation',\n",
              " 922: 'traub',\n",
              " 923: 'trial',\n",
              " 924: 'trials',\n",
              " 925: 'true',\n",
              " 926: 'truly',\n",
              " 927: 'tuning',\n",
              " 928: 'tures',\n",
              " 929: 'twice',\n",
              " 930: 'two',\n",
              " 931: 'tyagi',\n",
              " 932: 'typical',\n",
              " 933: 'typically',\n",
              " 934: 'uc',\n",
              " 935: 'underlying',\n",
              " 936: 'understand',\n",
              " 937: 'unfortunately',\n",
              " 938: 'uniform',\n",
              " 939: 'uniformly',\n",
              " 940: 'unique',\n",
              " 941: 'unit',\n",
              " 942: 'univ',\n",
              " 943: 'university',\n",
              " 944: 'unless',\n",
              " 945: 'upper',\n",
              " 946: 'upto',\n",
              " 947: 'us',\n",
              " 948: 'used',\n",
              " 949: 'useful',\n",
              " 950: 'using',\n",
              " 951: 'value',\n",
              " 952: 'values',\n",
              " 953: 'van',\n",
              " 954: 'variance',\n",
              " 955: 'vary',\n",
              " 956: 'vast',\n",
              " 957: 'vc',\n",
              " 958: 'vector',\n",
              " 959: 'vectors',\n",
              " 960: 'verify',\n",
              " 961: 'verlag',\n",
              " 962: 'versus',\n",
              " 963: 'via',\n",
              " 964: 'virtually',\n",
              " 965: 'vol',\n",
              " 966: 'volkan',\n",
              " 967: 'volume',\n",
              " 968: 'vs',\n",
              " 969: 'vybiral',\n",
              " 970: 'vybral',\n",
              " 971: 'wainwright',\n",
              " 972: 'wakin',\n",
              " 973: 'wasilkowski',\n",
              " 974: 'wasserman',\n",
              " 975: 'way',\n",
              " 976: 'weak',\n",
              " 977: 'weierstrass',\n",
              " 978: 'well',\n",
              " 979: 'whether',\n",
              " 980: 'white',\n",
              " 981: 'within',\n",
              " 982: 'without',\n",
              " 983: 'wk',\n",
              " 984: 'work',\n",
              " 985: 'works',\n",
              " 986: 'would',\n",
              " 987: 'wozniakowski',\n",
              " 988: 'write',\n",
              " 989: 'written',\n",
              " 990: 'xds',\n",
              " 991: 'xi',\n",
              " 992: 'xia',\n",
              " 993: 'xk',\n",
              " 994: 'xkf',\n",
              " 995: 'yi',\n",
              " 996: 'yields',\n",
              " 997: 'yj',\n",
              " 998: 'ykk',\n",
              " 999: 'york',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwzWGuTHukcS"
      },
      "outputs": [],
      "source": [
        "#We have 100 documents - apply doc2bow for each nd find term document frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "latwqAlV5_QT"
      },
      "outputs": [],
      "source": [
        "texts = data_words\n",
        "# Term Document Frequency\n",
        "#word frequency , kind of bag of words\n",
        "#doc frequency - frequency of a word in a document\n",
        "corpus = [id2word.doc2bow(text) for text in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auLOE9GCvHkT",
        "outputId": "09dc60fa-f3c9-4861-b281-184350f0df56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "len(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "data =['']\n",
        "\n",
        "# define the number of topics\n",
        "n_topics = 5\n",
        "\n",
        "# create a Latent Dirichlet Allocation model\n",
        "lda = LatentDirichletAllocation(n_components=n_topics)\n",
        "\n",
        "# fit the model to the data\n",
        "lda.fit(corpus)\n",
        "\n",
        "# transform the data using the fitted model\n",
        "transformed = lda.transform(corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "MAVz24WSpOhw",
        "outputId": "efa9a416-1187-4e9a-e617-936ede75cb45"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e9b7bdd6f048>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# fit the model to the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# transform the data using the fitted model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \"\"\"\n\u001b[1;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         X = self._check_non_neg_array(\n\u001b[0m\u001b[1;32m    642\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_n_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"LatentDirichletAllocation.fit\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_check_non_neg_array\u001b[0;34m(self, X, reset_n_features, whom)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreset_n_features\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_n_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy.array_api\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (100,) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLB6HcG3mB84",
        "outputId": "9af6b808-40b1-40ed-9ace-73c1950333ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.005*\"data\" + 0.004*\"learning\" + 0.004*\"algorithm\" + 0.004*\"time\" + '\n",
            "  '0.004*\"problem\" + 0.003*\"model\" + 0.003*\"set\" + 0.003*\"one\" + 0.003*\"two\" + '\n",
            "  '0.003*\"figure\"'),\n",
            " (1,\n",
            "  '0.006*\"model\" + 0.006*\"learning\" + 0.005*\"figure\" + 0.004*\"data\" + '\n",
            "  '0.004*\"set\" + 0.004*\"error\" + 0.004*\"function\" + 0.004*\"using\" + '\n",
            "  '0.003*\"algorithm\" + 0.003*\"results\"'),\n",
            " (2,\n",
            "  '0.005*\"model\" + 0.005*\"set\" + 0.005*\"learning\" + 0.005*\"data\" + '\n",
            "  '0.004*\"problem\" + 0.004*\"algorithm\" + 0.004*\"function\" + 0.003*\"using\" + '\n",
            "  '0.003*\"time\" + 0.003*\"two\"'),\n",
            " (3,\n",
            "  '0.006*\"learning\" + 0.005*\"model\" + 0.004*\"data\" + 0.004*\"set\" + '\n",
            "  '0.004*\"problem\" + 0.004*\"algorithm\" + 0.004*\"function\" + 0.003*\"using\" + '\n",
            "  '0.003*\"based\" + 0.003*\"models\"'),\n",
            " (4,\n",
            "  '0.008*\"data\" + 0.006*\"algorithm\" + 0.005*\"learning\" + 0.005*\"model\" + '\n",
            "  '0.004*\"using\" + 0.004*\"function\" + 0.004*\"set\" + 0.004*\"two\" + 0.004*\"time\" '\n",
            "  '+ 0.003*\"distribution\"'),\n",
            " (5,\n",
            "  '0.006*\"data\" + 0.005*\"model\" + 0.005*\"learning\" + 0.004*\"set\" + 0.004*\"two\" '\n",
            "  '+ 0.004*\"algorithm\" + 0.003*\"function\" + 0.003*\"problem\" + '\n",
            "  '0.003*\"distribution\" + 0.003*\"based\"'),\n",
            " (6,\n",
            "  '0.006*\"model\" + 0.005*\"using\" + 0.004*\"data\" + 0.004*\"learning\" + '\n",
            "  '0.004*\"function\" + 0.004*\"one\" + 0.004*\"set\" + 0.003*\"problem\" + '\n",
            "  '0.003*\"matrix\" + 0.003*\"time\"'),\n",
            " (7,\n",
            "  '0.006*\"model\" + 0.004*\"image\" + 0.004*\"learning\" + 0.004*\"data\" + '\n",
            "  '0.004*\"set\" + 0.004*\"function\" + 0.003*\"algorithm\" + 0.003*\"two\" + '\n",
            "  '0.003*\"one\" + 0.003*\"using\"'),\n",
            " (8,\n",
            "  '0.007*\"learning\" + 0.004*\"time\" + 0.004*\"model\" + 0.004*\"data\" + '\n",
            "  '0.004*\"one\" + 0.004*\"set\" + 0.004*\"function\" + 0.004*\"algorithm\" + '\n",
            "  '0.003*\"two\" + 0.003*\"figure\"'),\n",
            " (9,\n",
            "  '0.007*\"model\" + 0.005*\"data\" + 0.005*\"algorithm\" + 0.005*\"learning\" + '\n",
            "  '0.004*\"one\" + 0.004*\"using\" + 0.004*\"function\" + 0.004*\"time\" + '\n",
            "  '0.004*\"models\" + 0.004*\"number\"')]\n"
          ]
        }
      ],
      "source": [
        "#topic modelling\n",
        "from pprint import pprint\n",
        "from gensim.models import LdaMulticore\n",
        "topics = 10\n",
        "lda = LdaMulticore(corpus=corpus,num_topics=10,id2word=id2word)\n",
        "pprint(lda.print_topics())\n",
        "doc_lda = lda[corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHSKskcnxzmO",
        "outputId": "bb32571b-fa08-41a7-bb2e-856c0c72078f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([(4, 0.8120008), (5, 0.029328471), (8, 0.14246885), (9, 0.016046632)],\n",
              " [(0, 0.03162726),\n",
              "  (1, 0.047333833),\n",
              "  (3, 0.71878487),\n",
              "  (5, 0.024332521),\n",
              "  (8, 0.175485)],\n",
              " [(2, 0.61230314), (6, 0.011715809), (8, 0.36892992)],\n",
              " [(0, 0.9996587)],\n",
              " [(1, 0.99656606)],\n",
              " [(1, 0.47564095), (2, 0.010970898), (7, 0.10174423), (9, 0.4017746)],\n",
              " [(0, 0.5705442), (1, 0.41434905)],\n",
              " [(1, 0.03322549), (3, 0.14762959), (8, 0.8073722)],\n",
              " [(0, 0.011210617),\n",
              "  (1, 0.20481429),\n",
              "  (4, 0.029091502),\n",
              "  (6, 0.31133333),\n",
              "  (8, 0.4386979)],\n",
              " [(5, 0.9936849)],\n",
              " [(5, 0.9920083)],\n",
              " [(0, 0.43100125), (6, 0.5672779)],\n",
              " [(0, 0.13853782), (5, 0.86122274)],\n",
              " [(1, 0.09311963), (8, 0.8908781)],\n",
              " [(4, 0.16362636), (6, 0.05543881), (7, 0.78075206)],\n",
              " [(1, 0.024485186), (2, 0.21381505), (4, 0.7491012)],\n",
              " [(0, 0.10713105), (1, 0.016785666), (2, 0.010011742), (6, 0.8658207)],\n",
              " [(6, 0.9793841)],\n",
              " [(0, 0.042420436),\n",
              "  (1, 0.017207665),\n",
              "  (4, 0.036390003),\n",
              "  (5, 0.64052725),\n",
              "  (6, 0.2597086)],\n",
              " [(2, 0.9393772), (7, 0.060185328)],\n",
              " [(0, 0.016488941),\n",
              "  (2, 0.7591397),\n",
              "  (3, 0.059308082),\n",
              "  (6, 0.14417776),\n",
              "  (7, 0.016535424)],\n",
              " [(5, 0.031307507), (6, 0.9225998), (8, 0.045679156)],\n",
              " [(0, 0.12804049), (7, 0.053354893), (8, 0.81840146)],\n",
              " [(0, 0.33378893), (4, 0.65279937), (6, 0.013140822)],\n",
              " [(6, 0.06376995), (7, 0.37157053), (8, 0.5643495)],\n",
              " [(1, 0.039315857), (5, 0.9045033), (6, 0.045702197), (8, 0.01016653)],\n",
              " [(2, 0.13480344), (8, 0.07515366), (9, 0.7898194)],\n",
              " [(0, 0.09676759), (1, 0.011433534), (3, 0.8837782)],\n",
              " [(1, 0.96149606), (8, 0.034308553)],\n",
              " [(1, 0.084139384), (4, 0.52791566), (5, 0.35872513), (9, 0.0102099115)],\n",
              " [(5, 0.3166986), (6, 0.047502015), (7, 0.6264022)],\n",
              " [(5, 0.23999876), (6, 0.18298282), (8, 0.011499267), (9, 0.5652967)],\n",
              " [(0, 0.46508077), (2, 0.020374779), (5, 0.48740184), (8, 0.026860371)],\n",
              " [(2, 0.8238518), (6, 0.06465414), (8, 0.04099651), (9, 0.067424014)],\n",
              " [(0, 0.023414427),\n",
              "  (2, 0.22658867),\n",
              "  (4, 0.28726327),\n",
              "  (6, 0.057185374),\n",
              "  (7, 0.36489165),\n",
              "  (9, 0.040558465)],\n",
              " [(1, 0.24579825), (5, 0.06012655), (6, 0.010654829), (7, 0.6822326)],\n",
              " [(2, 0.9963535)],\n",
              " [(3, 0.08180096), (8, 0.91767263)],\n",
              " [(0, 0.019502351),\n",
              "  (1, 0.13787982),\n",
              "  (2, 0.5706564),\n",
              "  (3, 0.1555627),\n",
              "  (9, 0.10999578)],\n",
              " [(1, 0.018186329), (2, 0.16533828), (8, 0.8043657)],\n",
              " [(0, 0.9729454), (5, 0.026799327)],\n",
              " [(1, 0.04747421),\n",
              "  (4, 0.473901),\n",
              "  (5, 0.43367934),\n",
              "  (6, 0.010569189),\n",
              "  (7, 0.01681213),\n",
              "  (8, 0.013478894)],\n",
              " [(0, 0.8257543), (1, 0.15259817), (3, 0.014696679)],\n",
              " [(0, 0.87412375),\n",
              "  (1, 0.010366465),\n",
              "  (3, 0.0332555),\n",
              "  (5, 0.019177487),\n",
              "  (9, 0.062919505)],\n",
              " [(1, 0.992587)],\n",
              " [(1, 0.16653448),\n",
              "  (3, 0.1796156),\n",
              "  (5, 0.5483025),\n",
              "  (6, 0.050384108),\n",
              "  (8, 0.032345347),\n",
              "  (9, 0.010047576)],\n",
              " [(2, 0.2603901), (6, 0.73937833)],\n",
              " [(0, 0.04242151), (5, 0.9424537)],\n",
              " [(5, 0.99911815)],\n",
              " [(0, 0.47302783),\n",
              "  (5, 0.103601255),\n",
              "  (6, 0.14088424),\n",
              "  (7, 0.09347679),\n",
              "  (8, 0.015571507),\n",
              "  (9, 0.17005058)],\n",
              " [(0, 0.021991998),\n",
              "  (3, 0.017701363),\n",
              "  (4, 0.76982415),\n",
              "  (7, 0.14382139),\n",
              "  (8, 0.043616977)],\n",
              " [(1, 0.09879384),\n",
              "  (2, 0.76608986),\n",
              "  (4, 0.0669614),\n",
              "  (8, 0.0124936085),\n",
              "  (9, 0.055313893)],\n",
              " [(1, 0.014780462), (2, 0.044127602), (5, 0.11825595), (8, 0.821458)],\n",
              " [(0, 0.026742538), (8, 0.94867533), (9, 0.019815294)],\n",
              " [(1, 0.021609614),\n",
              "  (3, 0.015192921),\n",
              "  (5, 0.07120344),\n",
              "  (7, 0.8447285),\n",
              "  (8, 0.03910111)],\n",
              " [(0, 0.06311634), (7, 0.8917513), (8, 0.04198096)],\n",
              " [(1, 0.17991196), (3, 0.013160419), (4, 0.7849904), (7, 0.021692082)],\n",
              " [(3, 0.05949089), (5, 0.93188643)],\n",
              " [(6, 0.5483589), (8, 0.42249513), (9, 0.020800656)],\n",
              " [(8, 0.99351627)],\n",
              " [(8, 0.9986379)],\n",
              " [(0, 0.27971083),\n",
              "  (5, 0.18460083),\n",
              "  (6, 0.4267529),\n",
              "  (7, 0.07804796),\n",
              "  (9, 0.017422203)],\n",
              " [(1, 0.8542647), (2, 0.12329652), (7, 0.012195449)],\n",
              " [(8, 0.026567033), (9, 0.967343)],\n",
              " [(8, 0.99876016)],\n",
              " [(0, 0.95316315), (5, 0.033177078), (8, 0.013441366)],\n",
              " [(4, 0.858985), (6, 0.05108904), (7, 0.08792998)],\n",
              " [(5, 0.999524)],\n",
              " [(0, 0.869309), (7, 0.013497515), (8, 0.114284106)],\n",
              " [(5, 0.058396008), (7, 0.9321537)],\n",
              " [(2, 0.9939981)],\n",
              " [(7, 0.98098916)],\n",
              " [(4, 0.9201011), (6, 0.079477385)],\n",
              " [(0, 0.076955795), (6, 0.9147697)],\n",
              " [(1, 0.05392713), (3, 0.23311412), (8, 0.2672326), (9, 0.44419336)],\n",
              " [(1, 0.17409723), (4, 0.74778354), (8, 0.07763893)],\n",
              " [(4, 0.5450226), (7, 0.45431188)],\n",
              " [(0, 0.026882589),\n",
              "  (3, 0.35978818),\n",
              "  (6, 0.060606968),\n",
              "  (7, 0.5018669),\n",
              "  (9, 0.039370555)],\n",
              " [(1, 0.6050039), (6, 0.3946548)],\n",
              " [(8, 0.98808)],\n",
              " [(9, 0.9997351)],\n",
              " [(0, 0.025543215), (3, 0.019255541), (6, 0.6889265), (9, 0.250441)],\n",
              " [(5, 0.8625692), (9, 0.13506137)],\n",
              " [(0, 0.016368076), (1, 0.32558957), (7, 0.65131813)],\n",
              " [(0, 0.032561716), (1, 0.8868756), (9, 0.07816615)],\n",
              " [(5, 0.025159342), (9, 0.96859616)],\n",
              " [(2, 0.9997098)],\n",
              " [(0, 0.382947), (1, 0.114706025), (7, 0.029733397), (8, 0.47039518)],\n",
              " [(0, 0.084617786), (3, 0.87013406), (6, 0.028912371), (8, 0.013661329)],\n",
              " [(6, 0.99863523)],\n",
              " [(1, 0.029361598),\n",
              "  (3, 0.03312866),\n",
              "  (6, 0.30525708),\n",
              "  (8, 0.04417925),\n",
              "  (9, 0.587471)],\n",
              " [(0, 0.1976257),\n",
              "  (2, 0.05033147),\n",
              "  (4, 0.05536947),\n",
              "  (6, 0.53502774),\n",
              "  (7, 0.07298656),\n",
              "  (8, 0.07325787)],\n",
              " [(3, 0.3313797), (5, 0.6683939)],\n",
              " [(1, 0.013756814), (2, 0.9835511)],\n",
              " [(3, 0.98215836)],\n",
              " [(5, 0.07885374), (8, 0.91250014)],\n",
              " [(1, 0.09343885),\n",
              "  (2, 0.49296832),\n",
              "  (3, 0.1049723),\n",
              "  (7, 0.096359886),\n",
              "  (8, 0.21211311)],\n",
              " [(1, 0.99359083)],\n",
              " [(5, 0.95669544), (8, 0.04204723)],\n",
              " [(0, 0.037280515), (2, 0.37458378), (5, 0.029605914), (8, 0.55724627)])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tuple(doc_lda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "2j9OA5BNx5l_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "outputId": "dc8f362c-e108-4650-da7a-c1e370e7b015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BrokenProcessPool",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\nModuleNotFoundError: No module named 'pandas.core.indexes.numeric'\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-414466ffe1d3>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# # if you want to execute visualization prep yourself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mLDAvis_prepared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \"\"\"\n\u001b[1;32m    122\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0mterm_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m     topic_info = _topic_info(topic_term_dists, topic_proportion,\n\u001b[0m\u001b[1;32m    433\u001b[0m                              \u001b[0mterm_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                              n_jobs, start_index)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[1;32m    271\u001b[0m         ])\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     top_terms = pd.concat(Parallel(n_jobs=n_jobs)\n\u001b[0m\u001b[1;32m    274\u001b[0m                           (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)\n\u001b[1;32m    275\u001b[0m                           for ls in _job_chunks(lambda_seq, n_jobs)))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1697\u001b[0m             \u001b[0;31m# worker traceback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1699\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1700\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1732\u001b[0m         \u001b[0;31m# called directly or if the generator is gc'ed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1734\u001b[0;31m             \u001b[0merror_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_warn_exit_early\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;31m# callback thread, and is stored internally. It's just waiting to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;31m# be returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_or_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For other backends, the main thread needs to run the retrieval step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
          ]
        }
      ],
      "source": [
        "#visualizing\n",
        "import os\n",
        "import pyLDAvis.gensim\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(10))\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = pyLDAvis.gensim.prepare(topic_model = lda,corpus = corpus,dictionary =id2word)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft2hIOL4qHAa",
        "outputId": "be010164-7f47-4007-914b-78be3d92910c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.24.2 (from pyLDAvis)\n",
            "  Downloading numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.3)\n",
            "Collecting pandas>=2.0.0 (from pyLDAvis)\n",
            "  Downloading pandas-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.8.7)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
            "Collecting tzdata>=2022.1 (from pandas>=2.0.0->pyLDAvis)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Installing collected packages: funcy, tzdata, numpy, pandas, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed funcy-2.0 numpy-1.26.1 pandas-2.1.2 pyLDAvis-3.4.1 tzdata-2023.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xNeTvHbKqMRy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}